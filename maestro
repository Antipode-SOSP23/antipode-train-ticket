#!/usr/bin/env python3

from pathlib import Path
from pprint import pp
import os
import sys
import argparse
import time


#--------------
# REST REQUESTS
#--------------
def create_contact(user_id, auth_token):
  from faker import Faker
  from faker.providers import ssn, phone_number
  import requests

  fake = Faker()

  url = f"{_service_public_endpoint('ts-contacts-service')}/api/v1/contactservice/contacts"
  headers = { 'Authorization': 'Bearer ' + auth_token }
  params = {
    "accountId": user_id,
    "name": fake.name(),
    "documentNumber": fake.ssn(),
    "documentType": 1, # ID Card
    "phoneNumber": fake.phone_number()
  }
  r = requests.post(url, headers=headers, json=params)
  return r.json()

def login_user(username, password):
  import requests

  # Login user
  url = f"{_service_public_endpoint('ts-auth-service')}/api/v1/users/login"
  params = {
    "username": "fdse_microservice",
    "password": "111111",
    "verificationCode": ""
  }
  r = requests.post(url, json=params)
  if r.status_code != 200:
    print("[ERROR] Failed to login user")
    exit(-1)

  out = r.json()
  return out['data']['userId'], out['data']['token']

def cancel_order(order_id, user_id, auth_token):
  import requests

  url = f"{_service_public_endpoint('ts-cancel-service')}/api/v1/cancelservice/cancel/{order_id}/{user_id}"
  headers = { 'Authorization': 'Bearer ' + auth_token }
  r = requests.get(url, headers=headers)
  if r.status_code != 200:
    print("[ERROR] Failed to cancel order")
    exit(-1)
  return r

def fetch_drawback(auth_token):
  import requests

  url = f"{_service_public_endpoint('ts-inside-payment-service')}/api/v1/inside_pay_service/inside_payment/money"
  headers = { 'Authorization': 'Bearer ' + auth_token }
  r = requests.get(url, headers=headers)
  if r.status_code != 200:
    print("[ERROR] Failed to fetch drawback")
    exit(-1)

  # if len([e for e in r.json()['data'] if e['orderId'] == order_id ]) == 0:
  #   print("INCONSISTENT")
  # else:
  #   print("CONSISTENT")

  return r


#--------------
# INFO
#--------------
def info(args):
  if args['links']:
    public_ip = _public_hostname()
    print(f"Homepage:\thttp://{public_ip}:8080\t\t\t(fdse_microservice:111111:1234)")
    print(f"Admin:\t\thttp://{public_ip}:8080/adminlogin.html\t(admin:222222)")
    print(f"Jaeger:\t\thttp://{public_ip}:16686")
    print(f"RabbitMQ:\thttp://{public_ip}:15672\t\t\t(guest:guest)")
    print(f"API Docs:")
    print(f"\tts-auth-service:\t\t{_service_public_endpoint('ts-auth-service')}/swagger-ui.html#!/user-controller/")
    print(f"\tts-contacts-service:\t\t{_service_public_endpoint('ts-contacts-service')}/swagger-ui.html#/contacts-controller")
    print(f"\tts-order-service:\t\t{_service_public_endpoint('ts-order-service')}/swagger-ui.html")
    print(f"\tts-cancel-service:\t\t{_service_public_endpoint('ts-cancel-service')}/swagger-ui.html#!/cancel-controller")
    print(f"\tts-inside-payment-service:\t{_service_public_endpoint('ts-inside-payment-service')}/swagger-ui.html#/inside-payment-controller")

  if args['ps']:
    from plumbum import FG
    from plumbum.cmd import docker_compose

    os.chdir(APP_PATH)
    docker_compose['-f', DOCKER_COMPOSE_FILE, 'ps'] & FG

  if args['logs']:
    from plumbum import FG
    from plumbum.cmd import docker_compose

    os.chdir(APP_PATH)
    docker_compose['-f', DOCKER_COMPOSE_FILE, 'logs', args['logs']] & FG


#--------------
# BUILD
#--------------
def build(args):
  from plumbum import local, FG
  from plumbum.cmd import docker, docker_compose, find

  with local.cwd(APP_PATH):
    # remove target directories - maven was not cleaning
    find['.', '-type', 'd', '-name', 'target', '-exec', 'rm', '-rf', '{}', '+'] & FG
    # build from source
    # ref: https://github.com/FudanSELab/train-ticket/wiki/Installation-Guide
    mvn = docker['run', '--rm', '-ti',
      # keep cache between builds
      '-v', f"{os.environ['HOME']}/.m2:/root/.m2",
      # mount train ticket folder as working dir
      '-v', f"{APP_PATH}:/mnt/train-ticket",
      '-w', '/mnt/train-ticket',
      'maven:3-openjdk-8-slim',
      'mvn']
    mvn['clean', 'package', '-Dmaven.test.skip=true'] & FG
    time.sleep(1) # wait in between commands due to jar generation delay??
    docker_compose['build'] & FG
    if args['push']:
      docker_compose['push'] & FG


#--------------
# RUN
#--------------
def run(args):
  from plumbum import FG
  from plumbum import local
  from plumbum.cmd import docker_compose

  os.chdir(APP_PATH)

  with local.env(COMPOSE_HTTP_TIMEOUT=180):
    docker_compose['-f', DOCKER_COMPOSE_FILE, 'up'] & FG


#--------------
# CLEAN
#--------------
def clean(args):
  from plumbum import FG
  from plumbum.cmd import docker
  from plumbum.cmd import docker_compose

  os.chdir(APP_PATH)

  docker_compose['-f', DOCKER_COMPOSE_FILE, 'down'] & FG
  # cleaning system because train consumes a lot of memory/disk
  docker['system', 'prune', '--volumes', '-f'] & FG


#--------------
# WORKLOAD
#--------------
def wkld(args):
  from datetime import datetime
  from jinja2 import Environment
  import textwrap
  from plumbum import FG
  from plumbum import local
  from plumbum.cmd import docker_compose
  import shutil

  # tag for this deployment
  tag = f"{datetime.now().strftime('%Y%m%d%H%M%S')}"

  # create deploy folder
  deploy_path = ROOT_PATH / 'deploy' / tag
  os.makedirs(deploy_path, exist_ok=True)

  # copy locust files to deploy folder
  shutil.copy(ROOT_PATH / 'locust' / f"locustfile.py", deploy_path)
  shutil.copy(ROOT_PATH / 'locust' / f"Dockerfile", deploy_path)

  # dynamically build docker-compose for clients
  public_ip = _public_hostname()
  template = """
    version: '3'
    services:
      master:
        build: .
        working_dir: /mnt/locust
        command: -f locustfile.py --master -H http://master:8089 --expect-workers {{ num_workers }} --headless --reset-stats --only-summary --logfile {{ report_prefix }}.err --html {{ report_prefix }}.html --csv {{ report_prefix }} --run-time {{ run_time }} -u {{ users }}
        environment:
          AUTH_SERVICE_URL: {{ auth_service_url }}
          ORDER_SERVICE_URL: {{ order_service_url }}
          CANCEL_SERVICE_URL: {{ cancel_service_url }}
          INSIDE_PAYMENT_SERVICE_URL: {{ inside_payment_service_url }}
          USERNAME: {{ username }}
          PASSWORD: {{ password }}
          WORKER_ID: MASTER
          SEED_SIZE: {{ seed_size }}
        ports:
          - "8089:8089"
        volumes:
          - ./:/mnt/locust

      {% for worker_id in range(num_workers) %}
      worker_{{ worker_id }}:
        build: .
        working_dir: /mnt/locust
        command: -f locustfile.py --worker --master-host master --headless --reset-stats
        environment:
          AUTH_SERVICE_URL: {{ auth_service_url }}
          ORDER_SERVICE_URL: {{ order_service_url }}
          CANCEL_SERVICE_URL: {{ cancel_service_url }}
          INSIDE_PAYMENT_SERVICE_URL: {{ inside_payment_service_url }}
          USERNAME: {{ username }}
          PASSWORD: {{ password }}
          WORKER_ID: {{ worker_id }}
          SEED_SIZE: {{ seed_size }}
        volumes:
          - ./:/mnt/locust
      {% endfor %}
  """
  inventory = Environment().from_string(template).render({
    'auth_service_url': _service_public_endpoint('ts-auth-service'),
    'order_service_url': _service_public_endpoint('ts-order-service'),
    'cancel_service_url': _service_public_endpoint('ts-cancel-service'),
    'inside_payment_service_url': _service_public_endpoint('ts-inside-payment-service'),
    'username': 'fdse_microservice',
    'password': '111111',
    'num_workers': args['clients'],
    'users': args['threads'],
    'run_time': f"{args['duration']}s",
    'report_prefix': tag,
    'seed_size': SEED_TO_DURATION_FACTOR * args['duration'],
  })
  docker_compose_filepath = deploy_path / 'docker-compose.yml'
  with open(docker_compose_filepath, 'w') as f:
    # remove empty lines and dedent for easier read
    f.write(textwrap.dedent(inventory))
  print(f"[SAVED] '{docker_compose_filepath}'")

  # run locust and remove containers
  with local.cwd(deploy_path):
    docker_compose['up'] & FG
    # save log from master in
    (docker_compose['logs', '--no-color', '--no-log-prefix', 'master'] > f"master.out" )& FG
    # save log from workers
    for worker_id in range(args['clients']):
      (docker_compose['logs', '--no-color', '--no-log-prefix', f"worker_{worker_id}"] > f"worker_{worker_id}.out" )& FG
    # remove and clean containers
    docker_compose['down', '--rmi', 'local', '-v'] & FG

  # create gather folder
  gather_path = ROOT_PATH / 'gather' / tag
  os.makedirs(gather_path, exist_ok=True)
  # now move the gather files to the gather directory
  shutil.move(deploy_path / f"{tag}.err", gather_path / f"master_log.err")
  shutil.move(deploy_path / f"master.out", gather_path / f"master.out")
  for worker_id in range(args['clients']):
    shutil.move(deploy_path / f"worker_{worker_id}.out", gather_path / f"worker_{worker_id}.out")
  shutil.move(deploy_path / f"{tag}.html", gather_path / f"report.html")
  shutil.move(deploy_path / f"{tag}_exceptions.csv", gather_path / f"exceptions.csv")
  shutil.move(deploy_path / f"{tag}_failures.csv", gather_path / f"failures.csv")
  shutil.move(deploy_path / f"{tag}_stats_history.csv", gather_path / f"stats_history.csv")
  shutil.move(deploy_path / f"{tag}_stats.csv", gather_path / f"stats.csv")


#--------------
# HELPERS
#--------------
def _services():
  import yaml

  with open(DOCKER_COMPOSE_FILE) as file:
    return list(yaml.load(file, Loader=yaml.FullLoader)['services'].keys())

def _services_to_port():
  import yaml

  with open(DOCKER_COMPOSE_FILE) as file:
    return { name:int(d['ports'][0].split(':')[0]) for name,d in yaml.load(file, Loader=yaml.FullLoader)['services'].items() if 'ports' in d }

def _load_yaml(path):
  import yaml
  with open(path, 'r') as f:
    return yaml.safe_load(f) or {}

def _dump_yaml(path, d):
  import yaml
  with open(path, 'w+') as f:
    yaml.safe_dump(d, f, default_flow_style=False)

def _put_last(k,v):
  import yaml
  doc = {}
  # if file exists parse yaml otherwise create empty dict to write to
  if Path(LAST_INFO_FILE).exists():
    doc = _load_yaml(LAST_INFO_FILE)
  # write new value and save to file
  doc[k] = v
  _dump_yaml(LAST_INFO_FILE, doc)

def _get_last(k):
  import yaml
  doc = _load_yaml(LAST_INFO_FILE)
  return doc.get(k)

def _split_list(a, n):
    k, m = divmod(len(a), n)
    return list(a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))

def _public_hostname():
  from plumbum.cmd import hostname
  return hostname['-I']().split()[1]

def _service_public_endpoint(name):
  return f"http://{_public_hostname()}:{SERVICES_TO_PORT[name]}"

def _is_inside_docker():
  return os.path.isfile('/.dockerenv')

#--------------
# CONSTANTS
#--------------
ROOT_PATH = Path(os.path.abspath(os.path.dirname(sys.argv[0])))
APP_PATH = ROOT_PATH / 'train-ticket'
DOCKER_COMPOSE_FILE = APP_PATH / 'docker-compose.yml'
SERVICES = _services()
SERVICES_TO_PORT = _services_to_port()
LAST_INFO_FILE = ROOT_PATH / '.last.yml'
SEED_TO_DURATION_FACTOR = 50 # in 1 second it does less than this
DEPLOY_TYPES = [ 'local', 'gcp' ]


#--------------
# CLI
#--------------
if __name__ == '__main__':
  # parse arguments
  main_parser = argparse.ArgumentParser()

  # deploy flag
  deploy_type_group = main_parser.add_mutually_exclusive_group(required=False)
  for dt in DEPLOY_TYPES:
    deploy_type_group.add_argument(f"--{dt}", action='store_true')

  # different commands
  subparsers = main_parser.add_subparsers(help='commands', dest='which')

  # info application
  build_parser = subparsers.add_parser('build', help='Build application from source')
  build_parser.add_argument('-push', action='store_true', help="Push images to remote repo")

  # info application
  info_parser = subparsers.add_parser('info', help='Application info')
  info_parser.add_argument('-links', action='store_true', help="Application links")
  info_parser.add_argument('-ps', action='store_true', help="Application processes")
  info_parser.add_argument('-logs', choices=SERVICES, help="Application processes")

  # run application
  run_parser = subparsers.add_parser('run', help='Run application')

  # clean application
  clean_parser = subparsers.add_parser('clean', help='Clean application')

  # workload application
  seed_parser = subparsers.add_parser('seed', help='Seed orders')
  seed_parser.add_argument('-no', '--num-orders', type=int, default=1, help="Number or orders to spawn")

  # workload application
  wkld_parser = subparsers.add_parser('wkld', help='Run workload generator')
  wkld_parser.add_argument('-c', '--clients', type=int, default=1, help="Number of clients to run on")
  wkld_parser.add_argument('-t', '--threads', type=int, default=1, help="Number of threads")
  wkld_parser.add_argument('-d', '--duration', type=int, default='1', help="Duration in seconds")
  # wkld_parser.add_argument('-r', '--rate', type=int, default=1, help="Work rate (throughput) in request per second")

  # parse args
  args = vars(main_parser.parse_args())
  command = args.pop('which')

  # parse deploy type
  args['deploy_type'] = None
  for dt in DEPLOY_TYPES:
    if args[dt]:
      args['deploy_type'] = dt
    del args[dt]

  # only certain commands require a deploy type
  if args['deploy_type'] is None and command in ['run']:
    main_parser.error(f"one of the arguments {' '.join(['--'+dp for dp in DEPLOY_TYPES])} is required")

  # call parser method dynamically
  getattr(sys.modules[__name__], command)(args)