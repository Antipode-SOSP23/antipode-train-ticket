#!/usr/bin/env python3

from pathlib import Path
from pprint import pp
import os
import sys
import argparse
import time


#--------------
# GCP
#--------------
def _gcp_vm_create(zone, config):
  import googleapiclient.discovery
  import json

  compute = googleapiclient.discovery.build('compute', 'v1')
  try:
    compute.instances().insert(project=GCP_PROJECT_ID, zone=zone, body=config).execute()
    return _gcp_vm_wait_for_status(zone, config['name'], 'RUNNING')
  except googleapiclient.errors.HttpError as e:
    error_info = json.loads(e.args[1])['error']
    if error_info['code'] == 409 and error_info['errors'][0]['reason'] == 'alreadyExists':
      print(f"[WARN] Instance '{config['name']}' already exists")
      # get the existing instance details
      return _gcp_vm_get(zone=zone, name=config['name'])
    else:
      pp(error_info)
      exit(-1)

# Status available:
#   PROVISIONING, STAGING, RUNNING, STOPPING, SUSPENDING, SUSPENDED, REPAIRING, TERMINATED
def _gcp_vm_wait_for_status(zone, name, status):
  while True:
    r = _gcp_vm_get(zone=zone, name=name)
    if r['status'] == status.upper():
      return r

def _gcp_vm_get(zone, name):
  import googleapiclient.discovery
  compute = googleapiclient.discovery.build('compute', 'v1')
  return compute.instances().get(project=GCP_PROJECT_ID, zone=zone, instance=name).execute()

def _gcp_vm_wait_for_ip(zone, name):
  while True:
    metadata = _gcp_vm_get(zone, name)
    try:
      network_interface = metadata['networkInterfaces'][0]
      return network_interface['accessConfigs'][0]['natIP'], network_interface['networkIP']
    except KeyError:
      time.sleep(1)
      pass

def _gcp_vm_delete(zone, name):
  import googleapiclient.discovery
  import json

  compute = googleapiclient.discovery.build('compute', 'v1')
  try:
    compute.instances().delete(project=GCP_PROJECT_ID, zone=zone, instance=name).execute()
    # wait for delete to be completed
    while True:
      _gcp_vm_get(zone, name)
      time.sleep(1)
  except googleapiclient.errors.HttpError as e:
    error_info = json.loads(e.args[1])['error']
    if error_info['code'] == 404:
      return
    else:
      raise(e)

def _gcp_vm_start(zone, name):
  import googleapiclient.discovery
  import json

  compute = googleapiclient.discovery.build('compute', 'v1')
  while True:
    try:
      # start the vm if not running
      compute.instances().start(project=GCP_PROJECT_ID, zone=zone, instance=name).execute()
      # wait for delete to be completed
      return _gcp_vm_wait_for_status(zone, name, 'RUNNING')
    except googleapiclient.errors.HttpError as e:
      error_info = json.loads(e.args[1])['error']
      raise(e)


#--------------
# INFO
#--------------
def info(args):
  if args['links']:
    print(f"Homepage:\t{_service_public_endpoint(args['deploy_type'], 'ts-ui-dashboard')}\t\t\t({TT_USERNAME}:{TT_PASSWORD}:1234)")
    print(f"Admin:\t\t{_service_public_endpoint(args['deploy_type'], 'ts-ui-dashboard')}/adminlogin.html\t(admin:222222)")
    print(f"Jaeger:\t\t{_service_public_endpoint(args['deploy_type'], 'jaeger')}")
    print(f"RabbitMQ:\t{_service_public_endpoint(args['deploy_type'], 'rabbitmq')}\t\t\t(guest:guest)")
    print(f"Portainer:\t{_service_public_endpoint(args['deploy_type'], 'portainer')}\t\t\t(admin/antipode)")
    print(f"Prometheus:\t{_service_public_endpoint(args['deploy_type'], 'prometheus')}/graph?g0.expr=100%20-%20(avg%20by%20(instance)%20(irate(node_cpu_seconds_total%7Bjob%3D%22nodeexporter%22%2Cmode%3D%22idle%22%7D%5B5m%5D))%20*%20100)&g0.tab=0&g0.stacked=0&g0.range_input=10m&g0.step_input=1&g1.expr=(node_memory_MemTotal_bytes%20-%20node_memory_MemFree_bytes)%2Fnode_memory_MemTotal_bytes%20*100&g1.tab=0&g1.stacked=0&g1.range_input=10m&g1.step_input=1&g2.expr=(node_filesystem_size_bytes%7Bmountpoint%3D%22%2F%22%7D%20-%20node_filesystem_free_bytes%7Bmountpoint%3D%22%2F%22%7D)%2Fnode_filesystem_size_bytes%7Bmountpoint%3D%22%2F%22%7D%20*100&g2.tab=0&g2.stacked=0&g2.range_input=10m&g2.step_input=1&g3.expr=rate(node_network_transmit_bytes_total%7Bdevice%3D%22ens4%22%7D%5B10s%5D)*8%2F1024%2F1024&g3.tab=0&g3.stacked=0&g3.range_input=10m&g3.step_input=1")
    print(f"API Docs:")
    print(f"\tts-auth-service:\t\t{_service_public_endpoint(args['deploy_type'], 'ts-auth-service')}/swagger-ui.html#/user-controller/")
    print(f"\tts-contacts-service:\t\t{_service_public_endpoint(args['deploy_type'], 'ts-contacts-service')}/swagger-ui.html#/contacts-controller")
    print(f"\tts-order-service:\t\t{_service_public_endpoint(args['deploy_type'], 'ts-order-service')}/swagger-ui.html#/order-controller")
    print(f"\tts-cancel-service:\t\t{_service_public_endpoint(args['deploy_type'], 'ts-cancel-service')}/swagger-ui.html#/cancel-controller")
    print(f"\tts-inside-payment-service:\t{_service_public_endpoint(args['deploy_type'], 'ts-inside-payment-service')}/swagger-ui.html#/inside-payment-controller")

  if args['ps']:
    getattr(sys.modules[__name__], f"info__{args['deploy_type']}")(args)

  if args['logs']:
    getattr(sys.modules[__name__], f"info__{args['deploy_type']}")(args)

def info__local(args):
  tag = _get_last('local', 'deploy_tag')
  deploy_path = ROOT / 'deploy' / tag

  if args['ps']:
    from plumbum import local, FG
    from plumbum.cmd import docker_compose

    with local.cwd(APP_PATH):
      docker_compose[
        '--project-directory', APP_PATH,
        '-f', deploy_path / 'docker-compose.yml',
        'ps'
      ] & FG

  if args['logs']:
    from plumbum import local, FG
    from plumbum.cmd import docker_compose

    # service from train ticket
    if args['logs'] in TT_SERVICES:
      compose_args = [
        '--project-directory', APP_PATH,
        '-f', deploy_path / 'docker-compose.yml',
        'logs'
      ]
      if args['follow']: compose_args += ['--follow']
      compose_args += ['--tail=1000', args['logs']]
    elif args['logs'].startswith('worker') or args['logs'] == 'master':
      compose_args = ['-f', deploy_path / 'docker-compose-locust.yml', 'logs']
      if args['follow']: compose_args += ['--follow']
      compose_args += ['--tail=1000', args['logs']]

    docker_compose[compose_args] & FG

def info__gcp(args):
  _force_gcp_docker()

  inventory = _load_inventory(ROOT_PATH / 'deploy' / _get_last('gcp', 'deploy_tag') / 'inventory.cfg')
  config = _load_yaml(ROOT_PATH / _get_last('gcp', 'deploy_config'))

  if args['ps']:
    from plumbum import FG
    from plumbum.cmd import gcloud

    service_details = inventory[GCP_SWARM_MANAGER_NAME]

    gcloud['compute', 'ssh', f"root@{GCP_SWARM_MANAGER_NAME}",
      '--zone', service_details['gcp_zone'],
      # '--command', f"docker stack ps {GCP_SWARM_STACK_NAME}"
      '--command', f"docker stack services {GCP_SWARM_STACK_NAME}"
    ] & FG

  if args['logs']:
    from plumbum import FG
    from plumbum.cmd import gcloud

    follow_arg = '--follow' if args['follow'] else ''

    # service from train ticket
    if args['logs'] in TT_SERVICES:
      service_details = inventory[config['services'][args['logs']]]

      gcloud['compute', 'ssh', f"root@{GCP_SWARM_MANAGER_NAME}",
        '--zone', service_details['gcp_zone'],
        '--command', f"docker service logs --tail=1000 {follow_arg} {GCP_SWARM_STACK_NAME}_{args['logs']}"
      ] & FG
    elif args['logs'].startswith('seed'):
      service_details = inventory[GCP_SWARM_MANAGER_NAME]

      gcloud['compute', 'ssh', f"root@{GCP_SWARM_MANAGER_NAME}",
        '--zone', service_details['gcp_zone'],
        '--command', f"docker service logs --tail=1000 {follow_arg} seed_{args['logs']}"
      ] & FG
    elif args['logs'].startswith('worker') or args['logs'] == 'master':
      service_details = inventory[GCP_SWARM_MANAGER_NAME]

      gcloud['compute', 'ssh', f"root@{GCP_SWARM_MANAGER_NAME}",
        '--zone', service_details['gcp_zone'],
        '--command', f"docker service logs --tail=1000 {follow_arg} locust_{args['logs']}"
      ] & FG


#--------------
# BUILD
#--------------
def build(args):
  getattr(sys.modules[__name__], f"build__{args.pop('deploy_type')}")(args)
  print("[INFO] Build done!")

def build__local(args):
  from plumbum import local, FG
  from plumbum.cmd import docker, docker_compose, find

  with local.cwd(APP_PATH):
    # remove target directories - maven was not cleaning
    find['.', '-type', 'd', '-name', 'target', '-exec', 'rm', '-rf', '{}', '+'] & FG
    # build from source
    # ref: https://github.com/FudanSELab/train-ticket/wiki/Installation-Guide
    mvn = docker['run', '--rm', '-ti',
      # keep cache between builds
      '-v', f"{os.environ['HOME']}/.m2:/root/.m2",
      # mount train ticket folder as working dir
      '-v', f"{APP_PATH}:/mnt/train-ticket",
      '-w', '/mnt/train-ticket',
      'maven:3-openjdk-8-slim',
      'mvn']
    mvn['clean', 'package', '-Dmaven.test.skip=true'] & FG
    time.sleep(2) # wait in between commands due to jar generation delay??
    docker_compose['build'] & FG

  # build locust image
  with local.cwd(ROOT_PATH / 'wkld'):
    docker['build', '-t', LOCUST_DOCKER_IMAGE_NAME, '.'] & FG

def build__gcp(args):
  # build local images first
  if not _is_inside_docker():
    build__local(args)

  # then force docker
  _force_gcp_docker()
  import googleapiclient.discovery
  from plumbum import local, FG
  from plumbum.cmd import gcloud, docker
  from dotenv import dotenv_values
  import json

  # tag images built localy with GCP tag
  with local.cwd(APP_PATH):
    config = dotenv_values(".env")
    built_images = { s.split('|')[0] : s.split('|')[1] for s in docker['images', f"{config['NAMESPACE']}/*", '--format', "{{.ID}}|{{.Repository}}"]().split('\n') if s}
    for image_id, image_name in built_images.items():
      base_image_name = image_name.split('/')[-1]
      gcp_image_name = f"{GCP_DOCKER_IMAGE_NAMESPACE}/{base_image_name}:{GCP_DOCKER_IMAGE_TAG}"
      docker['tag', image_id, gcp_image_name] & FG
      docker['push', gcp_image_name] & FG
      docker['rmi', gcp_image_name] & FG

  # build and tag locust image
  with local.cwd(ROOT_PATH / 'wkld'):
    gcp_image_name = f"{GCP_DOCKER_IMAGE_NAMESPACE}/{LOCUST_DOCKER_IMAGE_NAME}"
    docker['tag', LOCUST_DOCKER_IMAGE_NAME, gcp_image_name] & FG
    docker['push', gcp_image_name] & FG
    docker['rmi', gcp_image_name] & FG

  # Create base VM
  compute = googleapiclient.discovery.build('compute', 'v1')
  image_response = compute.images().getFromFamily(project='debian-cloud', family='debian-10').execute()
  source_disk_image = image_response['selfLink']
  name = 'antipode-dev-tt'
  zone = 'us-east1-b'
  machine_type = 'f1-micro'
  config = {
    'name': name,
    'machineType': f"zones/{zone}/machineTypes/{machine_type}",
    'disks': [
      {
        'boot': True,
        'size': '20GB',
        'autoDelete': True,
        'initializeParams': {
          'sourceImage': source_disk_image,
        }
      }
    ],
    'hostname': 'antipode-dev.tt',
    # Specify a network interface with NAT to access the public internet.
    'networkInterfaces': [
      {
        'network': 'global/networks/default',
        'accessConfigs': [
          {'type': 'ONE_TO_ONE_NAT', 'name': 'External NAT'}
        ]
      }
    ],
    # tags for firewall rules
    "tags": {
      "items": [],
    },
  }
  r = _gcp_vm_create(zone, config)
  _gcp_vm_start(zone, r['id'])
  time.sleep(10) # extra time to avoid ssh errors

  # In order to get the .ssh folder
  # 1) Start a local GCP container
  # 2) Go the the GCP console, and in the SSH context menu, copy the gcloud command, it will look something like:
  #    $ gcloud beta compute ssh --zone "europe-west3-c" "antipode-dev-tt" --project "pluribus"
  # 3) Paste the above command in the container and let it run
  # 4) Copy the files from the container to outside
  #       docker cp <CONTAINER ID>:/root/.ssh/google_compute_engine .
  #       docker cp <CONTAINER ID>:/root/.ssh/google_compute_engine.pub .

  # copy and execute base_vm script
  gcloud['compute', 'scp', 'gcp/base_vm.sh', 'gcp/pluribus.json', f"{GCP_DEFAULT_SSH_USER}@{name}:/tmp/"] & FG
  gcloud['compute', 'ssh', f"{GCP_DEFAULT_SSH_USER}@{name}", '--command', 'bash /tmp/base_vm.sh'] & FG

  # stop machine
  compute.instances().stop(project=GCP_PROJECT_ID, zone=zone, instance=r['id']).execute()
  _gcp_vm_wait_for_status(zone, r['id'], 'TERMINATED')

  # built image based on the image of this machine
  image_config = {
    "kind": "compute#image",
    "name": GCP_MACHINE_IMAGE_NAME,
    "sourceDisk": "projects/pluribus/zones/us-east1-b/disks/antipode-dev-tt",
    "storageLocations": [
      "us"
    ]
  }

  # check if there is already an image - delete if necessary
  try:
    compute.images().get(project=GCP_PROJECT_ID, image=image_config['name']).execute()
    # if no exception delete the image
    print("[INFO] Image already exists - deleting!")
    compute.images().delete(project=GCP_PROJECT_ID, image=image_config['name']).execute()
    # wait for image to be deleted
    while True:
      # 404 exceptions will result into going to except and pass - means we deleted the image sucessfully
      compute.images().get(project=GCP_PROJECT_ID, image=image_config['name']).execute()
  except googleapiclient.errors.HttpError as e:
    error_info = json.loads(e.args[1])['error']
    if error_info['code'] == 404:
      pass
    else:
      raise(e)

  # create the new image
  compute.images().insert(project=GCP_PROJECT_ID, body=image_config).execute()
  # wait for image to be ready
  while True:
    if compute.images().get(project=GCP_PROJECT_ID, image=image_config['name']).execute()['status'] == 'READY':
      break

  # find all used ports
  firewall_rules = compute.firewalls().list(project=GCP_PROJECT_ID, filter=None).execute()['items']
  for frule in firewall_rules:
    if frule['name'] == 'portainer':
      print("[INFO] Firewall rule for portainer found")
    elif frule['name'] == 'swarm':
      print("[INFO] Firewall rule for swarm found")
    elif frule['name'] == 'nodes':
      print("[INFO] Firewall rule for nodes found")
      tcp_rule = [ e for e in frule['allowed'] if e['IPProtocol'] == 'tcp' ][0]
      ports = [ str(e) for e in sorted(set([ int(r) for r in tcp_rule['ports'] ])) ]
      # check ports
      docker_compose_ports = [ e.split(':')[0] for e in _flat_list([ sinfo['ports'] for sname, sinfo in _load_yaml(APP_PATH / 'docker-compose.yml')['services'].items() if 'ports' in sinfo]) ]
      if (set(docker_compose_ports) - set(ports)):
        print("[ERROR] Missing ports on firewall rule!")
        exit(-1)


#--------------
# DEPLOY
#--------------
def deploy(args):
  getattr(sys.modules[__name__], f"deploy__{args.pop('deploy_type')}")(args)
  print("[INFO] Deploy done!")

def deploy__local(args):
  return

def deploy__gcp(args):
  _force_gcp_docker()
  from plumbum import local, FG
  from plumbum.cmd import ansible_playbook
  from datetime import datetime
  from jinja2 import Environment
  import textwrap


  tag = f"{datetime.now().strftime('%Y%m%d%H%M%S')}"
  _set_last('gcp', 'deploy_tag', tag)

  # create deploy folder
  deploy_path = ROOT_PATH / 'deploy' / tag
  os.makedirs(deploy_path, exist_ok=True)

  # load the config and the local compose
  compose = _load_yaml(APP_PATH / 'docker-compose.yml')
  config = _load_yaml(args['config'])
  _set_last('gcp', 'deploy_config', args['config'])

  # Set external network driver
  compose['networks'][DOCKER_COMPOSE_NETWORK]['external'] = { 'name': DOCKER_COMPOSE_NETWORK }

  # replace the deploy host constraints
  for service, node_id in config['services'].items():
    sinfo = compose['services'][service]
    if 'deploy' not in sinfo: sinfo['deploy'] = {}
    if 'placement' not in sinfo['deploy']: sinfo['deploy']['placement'] = {}
    if 'constraints' not in sinfo['deploy']['placement']: sinfo['deploy']['placement']['constraints'] = []
    # get the index id of constraint of the node hostname
    deploy_constraints = sinfo['deploy']['placement']['constraints']
    hostname_index = _list_index_containing_substring(deploy_constraints, 'node.hostname')
    # replace hostname constraint
    deploy_constraints.insert(hostname_index, f"node.hostname == {node_id}")

  # replace container images
  for _, service_info in compose['services'].items():
    if 'image' in service_info:
      if '${NAMESPACE}' in service_info['image']:
        service_info['image'] = service_info['image'].replace('${NAMESPACE}', GCP_DOCKER_IMAGE_NAMESPACE).replace('${TAG}', GCP_DOCKER_IMAGE_TAG)

  # replace JVM opts in exclusive allocated nodes
  for node,services in _reverse_dict(config['services']).items():
    # filter services that have JAVA_OPTS
    services = [ s for s in services if 'environment' in compose['services'][s] and 'JAVA_OPTS' in compose['services'][s]['environment']]

    # if only 1 service with JAVA_OPTS in the host, then we can optimize JVM opts
    if len(services) < 2:
      compose['services'][services[0]]['environment']['JAVA_OPTS'] += ' '.join(GCP_INSTANCE_TO_JAVA_OPTS[config['nodes'][node]['machine_type']])

  # move all mongo instances to memory
  for m in [ n for n,s in compose['services'].items() if s['image'] == 'mongo' ]:
    service = compose['services'][m]
    service['command'] = ' '.join([
      # to avoid "slow query" messages
      "--slowms 999999",
      # for an in-memory database
      # ref: https://docs.mongodb.com/manual/reference/program/mongod/#std-option-mongod.--dbpath
      "--dbpath /ramdata",
      # `--syncdelay`: longer syncs with disk
      # ref: https://docs.mongodb.com/manual/reference/parameters/#mongodb-parameter-param.syncdelay
      "--syncdelay 0",
      # interval between syncs from memory to disk
      # https://docs.mongodb.com/manual/reference/program/mongod/#std-option-mongod.--journalCommitInterval
      "--journalCommitInterval 500",
      # "--wiredTigerCacheSizeGB 1",
    ])
    # volume for in-memory db
    if 'volumes' not in service:
      service['volumes'] = []
    service['volumes'].append({
      'type': 'tmpfs',
      'target': '/ramdata',
      'tmpfs': {
        'size': 3000000000,
      },
    })

  # now write the new compose into a new file
  _dump_yaml(deploy_path / 'docker-compose.yml', compose)
  print(f"[SAVED] '{deploy_path / 'docker-compose.yml'}'")

  # Keep track of all created vms
  vms = { 'swarm_manager': [], 'cluster': [], 'clients': [] }

  # manager
  manager_config = {
    'name': config['manager']['name'],
    'machineType': f"zones/{config['manager']['zone']}/machineTypes/{config['manager']['machine_type']}",
    'disks': [
      {
        'boot': True,
        'autoDelete': True,
        'initializeParams': {
          'sourceImage': GCP_MACHINE_IMAGE_LINK,
        }
      }
    ],
    'hostname': config['manager']['hostname'],
    # Specify a network interface with NAT to access the public internet.
    'networkInterfaces': [
      {
        'network': 'global/networks/default',
        'accessConfigs': [
          {'type': 'ONE_TO_ONE_NAT', 'name': 'External NAT'}
        ]
      }
    ],
    # tags for firewall rules
    "tags": {
      "items": ['swarm', 'portainer'],
    },
  }
  vms['swarm_manager'].append(_gcp_vm_create(config['manager']['zone'], manager_config))

  # app nodes
  for node_name, node_info in config['nodes'].items():
    node_config = {
      'name': node_name,
      'machineType': f"zones/{node_info['zone']}/machineTypes/{node_info['machine_type']}",
      'disks': [
        {
          'boot': True,
          'autoDelete': True,
          'initializeParams': {
            'sourceImage': GCP_MACHINE_IMAGE_LINK,
          }
        }
      ],
      'hostname': node_info['hostname'],
      # Specify a network interface with NAT to access the public internet.
      'networkInterfaces': [
        {
          'network': 'global/networks/default',
          'accessConfigs': [
            {'type': 'ONE_TO_ONE_NAT', 'name': 'External NAT'}
          ]
        }
      ],
      # tags for firewall rules
      "tags": {
        "items": ['swarm', 'nodes'],
      },
    }
    vms['cluster'].append(_gcp_vm_create(node_info['zone'], node_config))

  # client nodes
  for node_name, node_info in config['clients'].items():
    node_config = {
      'name': node_name,
      'machineType': f"zones/{node_info['zone']}/machineTypes/{node_info['machine_type']}",
      'disks': [
        {
          'boot': True,
          'autoDelete': True,
          'initializeParams': {
            'sourceImage': GCP_MACHINE_IMAGE_LINK,
          }
        }
      ],
      'hostname': node_info['hostname'],
      # Specify a network interface with NAT to access the public internet.
      'networkInterfaces': [
        {
          'network': 'global/networks/default',
          'accessConfigs': [
            {'type': 'ONE_TO_ONE_NAT', 'name': 'External NAT'}
          ]
        }
      ],
      # tags for firewall rules
      "tags": {
        "items": ['swarm', 'nodes'],
      },
    }
    vms['clients'].append(_gcp_vm_create(node_info['zone'], node_config))

  # Build inventory for ansible playbooks
  inventory = {}
  for group_name, group_nodes in vms.items():
    for v in group_nodes:
      external_ip, internal_ip = _gcp_vm_wait_for_ip(node_info['zone'], v['name'])

      if group_name not in inventory:
        inventory[group_name] = []
      inventory[group_name].append({
        'name': v['name'],
        'hostname': v['hostname'],
        'zone': v['zone'].split('/')[-1],
        'external_ip': external_ip,
        'internal_ip': internal_ip,
      })

  # build app ansible inventory
  # if you want to get a new google_compute_engine key:
  #   1) go to https://console.cloud.google.com/compute/instances
  #   2) choose one instance, click on SSH triangle for more options
  #   3) 'View gcloud command'
  #   4) Run that command and then copy the generated key
  #
  template = """
    [swarm_manager]
    {{ swarm_manager['hostname'] }} ansible_host={{ swarm_manager['external_ip'] }} gcp_zone={{ swarm_manager['zone'] }} gcp_name={{ swarm_manager['name'] }} gcp_host={{ swarm_manager['internal_ip'] }} ansible_user=root ansible_ssh_private_key_file={{ private_key_path }}

    [cluster]
    {% for node in cluster %}{{ node['hostname'] }} ansible_host={{ node['external_ip'] }} gcp_zone={{ node['zone'] }} gcp_name={{ node['name'] }} gcp_host={{ node['internal_ip'] }} ansible_user=root ansible_ssh_private_key_file={{ private_key_path }}
    {% endfor %}

    [clients]
    {% for node in clients %}{{ node['hostname'] }} ansible_host={{ node['external_ip'] }} gcp_zone={{ node['zone'] }} gcp_name={{ node['name'] }} gcp_host={{ node['internal_ip'] }} ansible_user=root ansible_ssh_private_key_file={{ private_key_path }}
    {% endfor %}
  """
  template_render = Environment().from_string(template).render({
    'swarm_manager': inventory['swarm_manager'][0], # only one swarm manager
    'cluster': inventory['cluster'],
    'clients': inventory['clients'],
    'private_key_path': str(ROOT_PATH / 'gcp' / '.ssh' / 'google_compute_engine'),
  })
  inventory_filepath = deploy_path / 'inventory.cfg'
  with open(inventory_filepath, 'w') as f:
    # remove empty lines and dedent for easier read
    f.write(textwrap.dedent(template_render))
  print(f"[SAVED] '{inventory_filepath}'")

  # sleep to give extra time for nodes to be ready to accept requests
  time.sleep(30)

  # run playbooks
  with local.cwd(ROOT_PATH / 'gcp'):
    # deploy swarm
    ansible_playbook['swarm-deploy.yml',
      '-i', inventory_filepath,
      '-e', f"swarm_overlay_network={DOCKER_COMPOSE_NETWORK}",
    ] & FG


#--------------
# RUN
#--------------
def run(args):
  _set_last(args['deploy_type'], 'drawback_delay', args['drawback_delay'])

  getattr(sys.modules[__name__], f"run__{args.pop('deploy_type')}")(args)
  print("[INFO] Run done!")

def run__local(args):
  from plumbum import local, FG
  from plumbum.cmd import docker_compose
  from datetime import datetime


  tag = f"{datetime.now().strftime('%Y%m%d%H%M%S')}"
  _set_last('local', 'deploy_tag', tag)

  # create deploy folder
  deploy_path = ROOT_PATH / 'deploy' / tag
  os.makedirs(deploy_path, exist_ok=True)

  # load existing docker-compose
  compose = _load_yaml(APP_PATH / 'docker-compose.yml')

  # edit the amount of delay in the docker-compose file
  if args['drawback_delay'] > 0:
    compose['services']['ts-cancel-service']['environment']['DELAY_DRAWBACK_SEC'] = args['drawback_delay']

  # move all mongo instances to memory
  for m in [ n for n,s in compose['services'].items() if s['image'] == 'mongo' ]:
    service = compose['services'][m]
    service['command'] = ' '.join([
      # to avoid "slow query" messages
      "--slowms 999999",
      # for an in-memory database
      # ref: https://docs.mongodb.com/manual/reference/program/mongod/#std-option-mongod.--dbpath
      "--dbpath /ramdata",
      # `--syncdelay`: longer syncs with disk
      # ref: https://docs.mongodb.com/manual/reference/parameters/#mongodb-parameter-param.syncdelay
      "--syncdelay 0",
      # interval between syncs from memory to disk
      # https://docs.mongodb.com/manual/reference/program/mongod/#std-option-mongod.--journalCommitInterval
      "--journalCommitInterval 500",
      # "--wiredTigerCacheSizeGB 1",
    ])
    # volume for in-memory db
    if 'volumes' not in service:
      service['volumes'] = []
    service['volumes'].append({
      'type': 'tmpfs',
      'target': '/ramdata',
      'tmpfs': {
        'size': 3000000000,
      },
    })

  # save (changed) docker-compose
  docker_compose_filepath = deploy_path / 'docker-compose.yml'
  _dump_yaml(docker_compose_filepath, compose)
  print(f"[SAVED] '{docker_compose_filepath}'")

  with local.cwd(APP_PATH):
    with local.env(COMPOSE_HTTP_TIMEOUT=180):
      if args['detached']:
        docker_compose[
          '--project-directory', APP_PATH,
          '-f', docker_compose_filepath,
          'up', '-d'
        ] & FG
        # wait for dashboard to be up
        _wait_url_up(_service_public_endpoint('local', 'ts-ui-dashboard'))
      else:
        docker_compose[
          '--project-directory', APP_PATH,
          '-f', docker_compose_filepath,
          'up'
        ] & FG

def run__gcp(args):
  _force_gcp_docker()
  from plumbum import local, FG
  from plumbum.cmd import ansible_playbook

  tag = _get_last('gcp', 'deploy_tag')
  deploy_path = ROOT_PATH / 'deploy' / tag
  inventory_filepath = deploy_path / 'inventory.cfg'
  inventory = _load_inventory(inventory_filepath)
  compose = _load_yaml(deploy_path / 'docker-compose.yml')

  # run playbooks
  if args['prometheus']:
    _set_last('gcp', 'prometheus', True)
    with local.cwd(ROOT_PATH / 'gcp'):
      ansible_playbook['prometheus-deploy.yml', '-i', inventory_filepath] & FG
      ansible_playbook['prometheus-run.yml', '-i', inventory_filepath] & FG
      _wait_url_up(f"http://{inventory[GCP_SWARM_MANAGER_NAME]['external_ip']}:9090")

  if args['portainer']:
    _set_last('gcp', 'portainer', True)
    with local.cwd(ROOT_PATH / 'gcp'):
      ansible_playbook['portainer-run.yml', '-i', inventory_filepath] & FG
      _wait_url_up(f"http://{inventory[GCP_SWARM_MANAGER_NAME]['external_ip']}:9000")

  if args['drawback_delay'] > 0:
    compose['services']['ts-cancel-service']['environment']['DELAY_DRAWBACK_SEC'] = args['drawback_delay']

  # save (changed) docker-compose
  docker_compose_filepath = deploy_path / 'docker-compose.yml'
  _dump_yaml(docker_compose_filepath, compose)
  print(f"[SAVED] '{docker_compose_filepath}'")

  # start train ticket services
  with local.cwd(ROOT_PATH / 'gcp'):
    ansible_playbook['tt-run.yml',
      '-i', inventory_filepath,
      '-e', f"deploy_path={deploy_path}",
      '-e', f"dashboard_url={_service_public_endpoint('gcp', 'ts-ui-dashboard')}",
      '-e', f"auth_url={_service_public_endpoint('gcp', 'ts-auth-service')}",
      '-e', f"order_url={_service_public_endpoint('gcp', 'ts-order-service')}",
      '-e', f"cancel_url={_service_public_endpoint('gcp', 'ts-cancel-service')}",
      '-e', f"payment_url={_service_public_endpoint('gcp', 'ts-inside-payment-service')}",
    ] & FG


#--------------
# CLEAN
#--------------
def clean(args):
  getattr(sys.modules[__name__], f"clean__{args.pop('deploy_type')}")(args)
  print("[INFO] Clean done!")

def clean__local(args):
  from plumbum import local, FG
  from plumbum.cmd import docker, docker_compose

  tag = _get_last('local', 'deploy_tag')
  deploy_path = ROOT_PATH / 'deploy' / tag

  # run locust and remove containers
  with local.cwd(deploy_path):
    # remove and clean containers
    if Path('docker-compose-locust.yml').exists():
      docker_compose['-f', 'docker-compose-locust.yml', 'down', '--rmi', 'local', '-v'] & FG
    if Path('docker-compose-seed.yml').exists():
      docker_compose['-f', 'docker-compose-seed.yml', 'down', '--rmi', 'local', '-v'] & FG

  with local.cwd(APP_PATH):
    if (deploy_path / 'docker-compose.yml').exists():
      docker_compose[
        '--project-directory', APP_PATH,
        '-f', deploy_path / 'docker-compose.yml',
        'down'
      ] & FG
    # cleaning system because train consumes a lot of memory/disk
    docker['system', 'prune', '--volumes', '-f'] & FG

def clean__gcp(args):
  _force_gcp_docker()
  from plumbum import local, FG
  from plumbum.cmd import ansible_playbook

  tag = _get_last('gcp', 'deploy_tag')
  deploy_path = ROOT_PATH / 'deploy' / tag
  inventory_filepath = deploy_path / 'inventory.cfg'
  inventory = _load_inventory(inventory_filepath)

  if _get_last('gcp', 'prometheus'):
    with local.cwd(ROOT_PATH / 'gcp'):
      ansible_playbook['prometheus-clean.yml', '-i', inventory_filepath] & FG
      _set_last('gcp', 'prometheus', False) # already cleaned so we remove flag

  if _get_last('gcp', 'portainer'):
    with local.cwd(ROOT_PATH / 'gcp'):
      ansible_playbook['portainer-clean.yml', '-i', inventory_filepath] & FG
      _set_last('gcp', 'portainer', False) # already cleaned so we remove flag

  with local.cwd(ROOT_PATH / 'gcp'):
    ansible_playbook['wkld-clean.yml', '-i', inventory_filepath] & FG
    ansible_playbook['tt-clean.yml', '-i', inventory_filepath] & FG
    ansible_playbook['swarm-clean.yml', '-i', inventory_filepath] & FG

  if args['strong']:
    print("[INFO] Removing GCP nodes ...")
    for _, host in inventory.items():
      _gcp_vm_delete(host['gcp_zone'], host['gcp_name'])


#--------------
# SEED
#--------------
def seed(args):
  getattr(sys.modules[__name__], f"seed__{args.pop('deploy_type')}")(args)
  print("[INFO] Seed done!")

def seed__local(args):
  import shutil
  from jinja2 import Environment
  import textwrap
  from plumbum import local, FG
  from plumbum.cmd import docker_compose, docker
  import click

  # tag for this deployment
  tag = _get_last('local', 'deploy_tag')
  deploy_path = ROOT_PATH / 'deploy' / tag

  # by default we restore from latest dump
  if not args['norestore']:
    # restore datastores dumps
    with local.cwd(deploy_path):
      for service_name in SEED_MONGO_INSTANCES:
        # get container id
        container_id = docker_compose[
          '--project-directory', APP_PATH,
          '-f', deploy_path / 'docker-compose.yml',
          'ps', '-q', service_name
        ]().rstrip()
        # copy file to container
        docker['cp',
          ROOT_PATH / 'wkld' / f"{service_name}.dump.tgz",
          f"{container_id}:/tmp/"
        ] & FG
        # restore dump
        docker_compose[
          '--project-directory', APP_PATH,
          '-f', deploy_path / 'docker-compose.yml',
          'exec', service_name,
          'mongorestore', '--objcheck', '--nsInclude=ts.*', '--gzip', f"--archive=/tmp/{service_name}.dump.tgz",
        ] & FG

  # build workers info
  workers = [ { 'id': f"worker{i}"} for i in range(args['clients']) ]

  # copy seed file to deploy folder
  shutil.copy(ROOT_PATH / 'wkld' / 'seed.py', deploy_path)

  # dynamically build docker-compose for client seeds
  template = """
    version: '3.6'

    services:
      {% for worker in workers %}
      {{ worker['id'] }}:
        image: {{ locust_docker_image_name }}
        working_dir: /mnt/locust
        command: python3 seed.py
        environment:
          DEPLOY_TAG: {{ deploy_tag }}
          WORKER_ID: {{ worker['id'] }}
          AUTH_SERVICE_URL: {{ auth_service_url }}
          ORDER_SERVICE_URL: {{ order_service_url }}
          INSIDE_PAYMENT_SERVICE_URL: {{ inside_payment_service_url }}
          TT_USERNAME: {{ tt_username }}
          TT_PASSWORD: {{ tt_password }}
          SEED_SIZE: {{ seed_size }}
        volumes:
          - ./:/mnt/locust
      {% endfor %}
  """
  template_render = Environment().from_string(template).render({
    'locust_docker_image_name': LOCUST_DOCKER_IMAGE_NAME,
    #
    'deploy_tag': tag,
    'auth_service_url': _service_public_endpoint('local', 'ts-auth-service'),
    'order_service_url': _service_public_endpoint('local', 'ts-order-service'),
    'inside_payment_service_url': _service_public_endpoint('local', 'ts-inside-payment-service'),
    'tt_username': TT_USERNAME,
    'tt_password': TT_PASSWORD,
    'workers': workers,
    'seed_size': args['size'],
  })
  docker_compose_filepath = deploy_path / 'docker-compose-seed.yml'
  with open(docker_compose_filepath, 'w') as f:
    # remove empty lines and dedent for easier read
    f.write(textwrap.dedent(template_render))
  print(f"[SAVED] '{docker_compose_filepath}'")

  # run seed
  with local.cwd(deploy_path):
    docker_compose['-f', f"{docker_compose_filepath}", 'up'] & FG

  # check if experiment was cleaned and if not we ask the user if he wants to continue
  if click.confirm('[WARN] Do you wish to replace existing seed file with dumped version?', default=True):
    with local.cwd(deploy_path):
      # mongodump for selected databases
      for service_name in SEED_MONGO_INSTANCES:
        dump_filename = f"{tag}_{service_name}.dump.tgz"
        # first execture the dump into a file
        docker_compose[
          '--project-directory', APP_PATH,
          '-f', deploy_path / 'docker-compose.yml',
          'exec', service_name,
          'mongodump', '--db=ts', '--gzip', f"--archive=/tmp/{dump_filename}",
        ] & FG
        # then get container id where the dump is
        container_id = docker_compose[
          '--project-directory', APP_PATH,
          '-f', deploy_path / 'docker-compose.yml',
          'ps', '-q', service_name
        ]().rstrip()
        # then copy file into local wkld folder
        docker['cp',
          f"{container_id}:/tmp/{dump_filename}",
          ROOT_PATH / 'wkld' / f"{service_name}.dump.tgz"
        ] & FG

  # remove and clean containers
  with local.cwd(deploy_path):
    docker_compose['-f', f"{docker_compose_filepath}", 'down', '--rmi', 'local', '-v'] & FG

        # now copy dumped files to the wkld dir
        shutil.copy(deploy_path / f"{tag}_{service_name}.dump.tgz", ROOT_PATH / 'wkld' / f"{service_name}.dump.tgz")


#--------------
# WORKLOAD
#--------------
def wkld(args):
  drawback_delay = _get_last(args['deploy_type'], 'drawback_delay')
  if drawback_delay is not None and drawback_delay > 0:
    print(f"[INFO] Drawback delay found: increasing wkld duration by {drawback_delay}s")
    args['duration'] += drawback_delay

  getattr(sys.modules[__name__], f"wkld__{args.pop('deploy_type')}")(args)
  print("[INFO] Workdload done!")

def wkld__local(args):
  from datetime import datetime
  from jinja2 import Environment
  import textwrap
  from plumbum import local, FG
  from plumbum.cmd import docker_compose, docker
  import shutil

  # tag for this deployment
  tag = _get_last('local', 'deploy_tag')
  deploy_path = ROOT_PATH / 'deploy' / tag

  # restore datastores dumps
  with local.cwd(deploy_path):
    for service_name in SEED_MONGO_INSTANCES:
      # get container id
      container_id = docker_compose[
        '--project-directory', APP_PATH,
        '-f', deploy_path / 'docker-compose.yml',
        'ps', '-q', service_name
      ]().rstrip()
      # copy file to container
      docker['cp',
        ROOT_PATH / 'wkld' / f"{service_name}.dump.tgz",
        f"{container_id}:/tmp/"
      ] & FG
      # restore dump
      docker_compose[
        '--project-directory', APP_PATH,
        '-f', deploy_path / 'docker-compose.yml',
        'exec', service_name,
        'mongorestore', '--objcheck', '--nsInclude=ts.*', '--gzip', f"--archive=/tmp/{service_name}.dump.tgz",
      ] & FG

  workers = [ { 'id': f"worker{i}"} for i in range(args['clients']) ]

  # split existing order ids per the number of clients
  order_ids = _index_orders('local')
  for worker,oids in zip(workers, _split_list(order_ids, len(workers))):
    oids_yaml_filepath = deploy_path / f"{tag}_seed_{worker['id']}.yml"
    _dump_yaml(oids_yaml_filepath, oids)
    print(f"[SAVED] '{oids_yaml_filepath}'")

  # copy locust files to deploy folder
  shutil.copy(ROOT_PATH / 'wkld' / 'locustfile.py', deploy_path)

  # dynamically build docker-compose for locust
  template = """
    version: '3.6'

    services:
      master:
        image: {{ locust_docker_image_name }}
        working_dir: /mnt/locust
        command: locust -f locustfile.py --master --master-bind-host=0.0.0.0 --master-bind-port=8089 --host http://master:8089 --expect-workers {{ workers | length }} --headless --reset-stats --only-summary --logfile {{ deploy_tag }}.err --html {{ deploy_tag }}.html --csv {{ deploy_tag }} --run-time {{ run_time }} -u {{ users }}
        environment:
          DEPLOY_TAG: {{ deploy_tag }}
          WORKER_ID: MASTER
          AUTH_SERVICE_URL: {{ auth_service_url }}
          CANCEL_SERVICE_URL: {{ cancel_service_url }}
          INSIDE_PAYMENT_SERVICE_URL: {{ inside_payment_service_url }}
          TT_USERNAME: {{ tt_username }}
          TT_PASSWORD: {{ tt_password }}
          RATE: {{ rate }}
        ports:
          - 8089:8089
        volumes:
          - ./:/mnt/locust

      {% for worker in workers %}
      {{ worker['id'] }}:
        image: {{ locust_docker_image_name }}
        working_dir: /mnt/locust
        command: locust -f locustfile.py --worker --master-host master --master-port=8089 --headless --reset-stats
        environment:
          DEPLOY_TAG: {{ deploy_tag }}
          WORKER_ID: {{ worker['id'] }}
          AUTH_SERVICE_URL: {{ auth_service_url }}
          CANCEL_SERVICE_URL: {{ cancel_service_url }}
          INSIDE_PAYMENT_SERVICE_URL: {{ inside_payment_service_url }}
          TT_USERNAME: {{ tt_username }}
          TT_PASSWORD: {{ tt_password }}
          RATE: {{ rate }}
        volumes:
          - ./:/mnt/locust
      {% endfor %}
  """
  template_render = Environment().from_string(template).render({
    'locust_docker_image_name': LOCUST_DOCKER_IMAGE_NAME,
    #
    'deploy_tag': tag,
    'auth_service_url': _service_public_endpoint('local', 'ts-auth-service'),
    'cancel_service_url': _service_public_endpoint('local', 'ts-cancel-service'),
    'inside_payment_service_url': _service_public_endpoint('local', 'ts-inside-payment-service'),
    'tt_username': TT_USERNAME,
    'tt_password': TT_PASSWORD,
    'workers': workers,
    'users': args['threads'],
    'run_time': f"{args['duration']}s",
    'rate': args['rate'],
  })
  docker_compose_filepath = deploy_path / 'docker-compose-locust.yml'
  with open(docker_compose_filepath, 'w') as f:
    # remove empty lines and dedent for easier read
    f.write(textwrap.dedent(template_render))
  print(f"[SAVED] '{docker_compose_filepath}'")

  # run locust and remove containers
  with local.cwd(deploy_path):
    docker_compose['-f', f"{docker_compose_filepath}", 'up'] & FG
    # save log from master in
    (docker_compose['-f', f"{docker_compose_filepath}", 'logs', '--no-color', '--no-log-prefix', 'master'] > f"{tag}_master.out" )& FG
    # save log from workers
    for worker in workers:
      (docker_compose['-f', f"{docker_compose_filepath}", 'logs', '--no-color', '--no-log-prefix', f"{worker['id']}"] > f"{tag}_{worker['id']}.out" )& FG
    # remove and clean containers
    docker_compose['-f', f"{docker_compose_filepath}", 'down', '--rmi', 'local', '-v'] & FG

  # create gather folder
  gather_path = ROOT_PATH / 'gather' / tag
  os.makedirs(gather_path, exist_ok=True)

  # now move the gather files to the gather directory
  shutil.move(deploy_path / f"{tag}.err", gather_path / f"master_log.err")
  shutil.move(deploy_path / f"{tag}_master.out", gather_path / f"master.out")
  for worker in workers:
    shutil.move(deploy_path / f"{tag}_{worker['id']}.out", gather_path / f"{worker['id']}.out")
  shutil.move(deploy_path / f"{tag}.html", gather_path / f"report.html")
  shutil.move(deploy_path / f"{tag}_exceptions.csv", gather_path / f"exceptions.csv")
  shutil.move(deploy_path / f"{tag}_failures.csv", gather_path / f"failures.csv")
  shutil.move(deploy_path / f"{tag}_stats_history.csv", gather_path / f"stats_history.csv")
  shutil.move(deploy_path / f"{tag}_stats.csv", gather_path / f"stats.csv")

  # duplicate last file with run tags
  _dump_yaml(gather_path / 'last.yml', _load_yaml(LAST_INFO_FILE)['local'])

def wkld__gcp(args):
  _force_gcp_docker()
  import shutil
  from jinja2 import Environment
  import textwrap
  from plumbum import local, FG
  from plumbum.cmd import docker, docker_compose, ansible_playbook


  tag = _get_last('gcp', 'deploy_tag')
  deploy_path = ROOT_PATH / 'deploy' / tag
  config = _load_yaml(ROOT_PATH / _get_last('gcp', 'deploy_config'))
  inventory = _load_inventory(deploy_path / 'inventory.cfg')

  if len(config['clients']) < args['clients']:
    print(f"[ERROR] Config '{_get_last('gcp', 'deploy_config')}' has less clients ({len(config['clients'])}) than cli args ({args['clients']})")
    exit(-1)
  # select clients according to wanted run
  clients_inventory = { i:inventory[i] for i in list(config['clients'].keys())[:args['clients']] }

  # copy locust file to deploy folder
  shutil.copy(ROOT_PATH / 'wkld' / 'locustfile.py', deploy_path)

  # dynamically build docker-compose for clients
  template = """
    version: '3.6'

    networks:
      {{ docker_compose_network }}:
        external:
          name: {{ docker_compose_network }}

    services:
      master:
        image: {{ gcp_docker_image_namespace }}/{{ locust_docker_image_name }}
        working_dir: /mnt/locust
        command: locust -f locustfile.py --master --master-bind-host=0.0.0.0 --master-bind-port=8089 --host http://master:8089 --expect-workers {{ workers | length }} --headless --reset-stats --only-summary --logfile {{ deploy_tag }}.err --html {{ deploy_tag }}.html --csv {{ deploy_tag }} --run-time {{ run_time }} -u {{ users }}
        environment:
          DEPLOY_TAG: {{ deploy_tag }}
          WORKER_ID: MASTER
          AUTH_SERVICE_URL: {{ auth_service_url }}
          CANCEL_SERVICE_URL: {{ cancel_service_url }}
          INSIDE_PAYMENT_SERVICE_URL: {{ inside_payment_service_url }}
          TT_USERNAME: {{ tt_username }}
          TT_PASSWORD: {{ tt_password }}
          RATE: {{ rate }}
        ports:
          - 8089:8089
        volumes:
          - /code/wkld:/mnt/locust
        networks:
          {{ docker_compose_network }}:
            aliases:
              - master
        deploy:
          restart_policy:
            # do not restart when ends correctly
            condition: on-failure
          placement:
            constraints:
              - node.hostname == manager

      {% for _,worker in workers.items() %}
      {{ worker['gcp_name'] }}:
        image: {{ gcp_docker_image_namespace }}/{{ locust_docker_image_name }}
        working_dir: /mnt/locust
        command: locust -f locustfile.py --worker --master-host master --master-port=8089 --headless --reset-stats
        environment:
          DEPLOY_TAG: {{ deploy_tag }}
          WORKER_ID: {{ worker['gcp_name'] }}
          AUTH_SERVICE_URL: {{ auth_service_url }}
          CANCEL_SERVICE_URL: {{ cancel_service_url }}
          INSIDE_PAYMENT_SERVICE_URL: {{ inside_payment_service_url }}
          TT_USERNAME: {{ tt_username }}
          TT_PASSWORD: {{ tt_password }}
          RATE: {{ rate }}
        volumes:
          - /code/wkld:/mnt/locust
        networks:
          {{ docker_compose_network }}:
            aliases:
              - {{ worker['gcp_name'] }}
        deploy:
          restart_policy:
            # do not restart when ends correctly
            condition: on-failure
          placement:
            constraints:
              - node.hostname == {{ worker['gcp_name'] }}
      {% endfor %}
  """
  template_render = Environment().from_string(template).render({
    'gcp_docker_image_namespace': GCP_DOCKER_IMAGE_NAMESPACE,
    'locust_docker_image_name': LOCUST_DOCKER_IMAGE_NAME,
    'docker_compose_network': DOCKER_COMPOSE_NETWORK,
    'workers': clients_inventory,
    'deploy_tag': tag,
    #
    'auth_service_url': _service_public_endpoint('gcp', 'ts-auth-service'),
    'cancel_service_url': _service_public_endpoint('gcp', 'ts-cancel-service'),
    'inside_payment_service_url': _service_public_endpoint('gcp', 'ts-inside-payment-service'),
    'tt_username': TT_USERNAME,
    'tt_password': TT_PASSWORD,
    'users': args['threads'],
    'run_time': f"{args['duration']}s",
    'rate': args['rate'],
  })
  docker_compose_filepath = deploy_path / 'docker-compose-locust.yml'
  with open(docker_compose_filepath, 'w') as f:
    # remove empty lines and dedent for easier read
    f.write(textwrap.dedent(template_render))
  print(f"[SAVED] '{docker_compose_filepath}'")

  # now we run the playbook
  with local.cwd(ROOT_PATH / 'gcp'):
    ansible_playbook['wkld-run.yml',
      '-i', deploy_path / 'inventory.cfg',
      '-e', f"deploy_path={deploy_path}",
      '-e', f"deploy_tag={tag}",
      '-e', f"duration={args['duration']}",
      '-e', f"num_workers={len(clients_inventory)}",
    ] & FG

    # create gather folder
    gather_path = ROOT_PATH / 'gather' / tag
    os.makedirs(gather_path, exist_ok=True)

    ansible_playbook['wkld-gather.yml',
      '-i', deploy_path / 'inventory.cfg',
      '-e', f"deploy_path={deploy_path}",
      '-e', f"gather_path={gather_path}",
      '-e', f"deploy_tag={tag}",
    ] & FG

  # duplicate last file with run tags
  _dump_yaml(gather_path / 'last.yml', _load_yaml(LAST_INFO_FILE)['gcp'])


#--------------
# SSH
#--------------
def ssh(args):
  getattr(sys.modules[__name__], f"ssh__{args.pop('deploy_type')}")(args)
  print("[INFO] SSH done!")

def ssh__local(args):
  return

def ssh__gcp(args):
  from plumbum import FG
  from plumbum.cmd import xpanes, ssh

  tag = _get_last('gcp', 'deploy_tag')
  deploy_path = ROOT_PATH / 'deploy' / tag
  inventory_filepath = deploy_path / 'inventory.cfg'
  inventory = _load_inventory(inventory_filepath)
  ssh_key = ROOT_PATH / 'gcp' / '.ssh' / 'google_compute_engine'

  xpanes_args = ['xpanes', '-c', f"'ssh -o StrictHostKeyChecking=no -i {ssh_key} root@{{}}'"]
  for h in args['hosts']:
    if h not in inventory:
      print(f"[WARN] Skipping '{h}' - not found in inventory")
    # get the public ip
    xpanes_args.append(inventory[h]['external_ip'])

  # plumbum does not work hence
  os.system(' '.join(xpanes_args))


#--------------
# HELPERS
#--------------
def _services():
  import yaml

  with open(APP_PATH / 'docker-compose.yml') as file:
    return list(yaml.load(file, Loader=yaml.FullLoader)['services'].keys())

def _services_to_port():
  import yaml

  with open(APP_PATH / 'docker-compose.yml') as file:
    return { name:int(d['ports'][0].split(':')[0]) for name,d in yaml.load(file, Loader=yaml.FullLoader)['services'].items() if 'ports' in d }

def _index_orders(deploy_type):
  import requests
  r = requests.get(f"{_service_public_endpoint(deploy_type, 'ts-order-service')}/api/v1/orderservice/order")
  return [ o['id'] for o in r.json()['data'] ]

def _load_yaml(path):
  import yaml
  with open(path, 'r') as f:
    return yaml.safe_load(f) or {}

def _dump_yaml(path, d):
  import yaml
  with open(path, 'w+') as f:
    yaml.safe_dump(d, f, default_flow_style=False)

def _set_last(deploy_type, k,v):
  doc = {}
  # if file exists parse yaml otherwise create empty dict to write to
  if Path(LAST_INFO_FILE).exists():
    doc = _load_yaml(LAST_INFO_FILE)
  # write new value and save to file
  if deploy_type not in doc:
    doc[deploy_type] = {}
  doc[deploy_type][k] = v
  _dump_yaml(LAST_INFO_FILE, doc)

def _get_last(deploy_type, k):
  doc = _load_yaml(LAST_INFO_FILE)
  return doc.get(deploy_type).get(k)

def _split_list(a, n):
    k, m = divmod(len(a), n)
    return list(a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))

def _flat_list(l):
  return [item for sublist in l for item in sublist]

def _list_index_containing_substring(the_list, substring):
  for i, s in enumerate(the_list):
    if substring in s:
      return i
  return 0

def _service_public_endpoint(deploy_type, name):
  public_ip, port = getattr(sys.modules[__name__], f"_service_public_endpoint__{deploy_type}")(name)
  return f"http://{public_ip}:{port}"

def _service_public_endpoint__local(name):
  from plumbum.cmd import hostname

  public_ip = hostname['-I']().split()[1]
  if name == 'portainer':
    return public_ip, 9000
  elif name == 'prometheus':
    return public_ip, 9090
  else:
    return public_ip, TT_SERVICES_TO_PORT[name]

def _service_public_endpoint__gcp(name):
  inventory = _load_inventory(ROOT_PATH / 'deploy' / _get_last('gcp', 'deploy_tag') / 'inventory.cfg')
  config = _load_yaml(ROOT_PATH / _get_last('gcp', 'deploy_config'))

  if name == 'portainer':
    return inventory[GCP_SWARM_MANAGER_NAME]['external_ip'], 9000
  elif name == 'prometheus':
    return inventory[GCP_SWARM_MANAGER_NAME]['external_ip'], 9090
  else:
    return inventory[config['services'][name]]['external_ip'], TT_SERVICES_TO_PORT[name]

def _is_inside_docker():
  return os.path.isfile('/.dockerenv')

def _force_gcp_docker():
  if _is_inside_docker():
    return

  from plumbum import local, FG
  from plumbum.cmd import docker
  import subprocess

  # if image is not built, we do it
  if bool(int(os.environ.get('REBUILD_GCP_DOCKER_IMAGE',0))) or docker['images', GCP_DOCKER_IMAGE_NAME, '--format', '"{{.ID}}"']().strip() == '':
    with local.cwd(ROOT_PATH / 'gcp'):
      docker['build', '-t', GCP_DOCKER_IMAGE_NAME, '.'] & FG

  args = list()
  args.extend(['docker', 'run', '--rm', '-it',
    # run docker from host inside the container
    '-v', '/var/run/docker.sock:/var/run/docker.sock',
    '-v', '/usr/bin/docker:/usr/bin/docker',
    # volumes
    '-v', f"{ROOT_PATH}:/mnt/code",
    '-v', f"{ROOT_PATH / 'gcp' / '.ssh'}:/root/.ssh",
    '-w', '/mnt/code',
    #
    GCP_DOCKER_IMAGE_NAME
  ])
  args = args + sys.argv
  # print(' '.join(args))
  subprocess.call(args)
  exit()

def _wait_url_up(url):
  import urllib.request

  while True:
    try:
      return_code = urllib.request.urlopen(url).getcode()
      if return_code == 200:
        return
    except urllib.error.URLError as e:
      pass

def _load_inventory(filepath):
  from ansible.parsing.dataloader import DataLoader
  from ansible.inventory.manager import InventoryManager
  from ansible.vars.manager import VariableManager

  loader = DataLoader()
  inventory = InventoryManager(loader=loader, sources=str(filepath))
  variable_manager = VariableManager(loader=loader, inventory=inventory)

  # other methods:
  # - inventory.get_host('manager.antipode-296620').vars

  loaded_inventory = {}
  for gcp_hostname,info in inventory.hosts.items():
    # index by service name
    e = {}
    # load hostname manually since its the key for each host
    e['gcp_hostname'] = gcp_hostname
    # load all remaining keys
    for k,v in info.vars.items():
      e[k] = v
    # refactor some entries
    e['external_ip'] = e['ansible_host']
    e['internal_ip'] = e['gcp_host']
    del e['ansible_host']
    del e['gcp_host']
    # assign new entry
    loaded_inventory[info.vars['gcp_name']] = e

  return loaded_inventory

def _reverse_dict(d):
  rev = {}
  for k,v in d.items():
    if v not in rev:
      rev[v] = []
    rev[v].append(k)
  return rev


#--------------
# CONSTANTS
#--------------
SCRIPT_NAME = __file__.rsplit("/", 1)[1].split('.')[0]
ROOT_PATH = Path(os.path.abspath(os.path.dirname(sys.argv[0])))
APP_PATH = ROOT_PATH / 'train-ticket'
DOCKER_COMPOSE_NETWORK = 'my-network'
TT_SERVICES = _services()
TT_SERVICES_TO_PORT = _services_to_port()
TT_USERNAME = 'fdse_microservice'
TT_PASSWORD = '111111'
LAST_INFO_FILE = ROOT_PATH / '.last.yml'
LOCUST_DOCKER_IMAGE_NAME = 'locust-tt:antipode'
SEED_MONGO_INSTANCES = ['ts-order-mongo', 'ts-inside-payment-mongo']
SEED_TO_DURATION_FACTOR = 35 # in 1 second it does less than this
DEPLOY_TYPES = [ 'local', 'gcp' ]
GCP_PROJECT_ID = 'pluribus'
GCP_DOCKER_IMAGE_NAME = 'gcp-manager:antipode'
GCP_DOCKER_IMAGE_NAMESPACE = f"gcr.io/{GCP_PROJECT_ID}"
GCP_DOCKER_IMAGE_TAG = "antipode"
GCP_DEFAULT_SSH_USER = 'jfloff'
GCP_MACHINE_IMAGE_NAME = 'antipode-tt'
GCP_MACHINE_IMAGE_LINK = f"https://www.googleapis.com/compute/v1/projects/{GCP_PROJECT_ID}/global/images/{GCP_MACHINE_IMAGE_NAME}"
GCP_SWARM_MANAGER_NAME = 'manager-tt'
GCP_SWARM_STACK_NAME = 'train-ticket'
GCP_INSTANCE_TO_JAVA_OPTS = {
  "e2-standard-4": [
    "-Xms5g", # 30% of 16Gb
    "-XX:ParallelGCThreads=4",
    "-XX:ConcGCThreads=4",
    "-XX:MaxGCPauseMillis=100",
  ],
  "e2-standard-8": [
    "-Xms10g", # 30% of 32Gb
    # "-Xmx13500M",
    "-XX:ParallelGCThreads=8",
    "-XX:ConcGCThreads=8",
    "-XX:MaxGCPauseMillis=100"
  ],
}


#--------------
# CLI
#--------------
if __name__ == '__main__':
  # parse arguments
  main_parser = argparse.ArgumentParser()

  # deploy flag
  deploy_type_group = main_parser.add_mutually_exclusive_group(required=True)
  for dt in DEPLOY_TYPES:
    deploy_type_group.add_argument(f"--{dt}", action='store_true')

  # different commands
  subparsers = main_parser.add_subparsers(help='commands', dest='which')

  # build application
  build_parser = subparsers.add_parser('build', help='Build application from source')

  # deploy application
  deploy_parser = subparsers.add_parser('deploy', help='Deploy application')
  deploy_parser.add_argument('-config', help="Deploy configuration")

  # info application
  info_parser = subparsers.add_parser('info', help='Application info')
  info_parser.add_argument('-links', action='store_true', help="Application links")
  info_parser.add_argument('-ps', action='store_true', help="Application processes")
  info_parser.add_argument('-logs', help="Application processes")
  info_parser.add_argument('-follow', action='store_true', help="Follow output")

  # run application
  run_parser = subparsers.add_parser('run', help='Run application')
  run_parser.add_argument('-prometheus', action='store_true', help="Run with prometheus enabled")
  run_parser.add_argument('-portainer', action='store_true', help="Run with portainer enabled")
  run_parser.add_argument('-detached', action='store_true', help="Run in detach mode")
  run_parser.add_argument('-drawback-delay', required=False, type=int, default=-1, help="Add delay to drawback on cancel action")

  # clean application
  clean_parser = subparsers.add_parser('clean', help='Clean application')
  clean_parser.add_argument('-strong', action='store_true', help="Strong clean deleting containers or remote nodes")

  # seed application
  seed_parser = subparsers.add_parser('seed', help='Seed application')
  seed_parser.add_argument('-norestore', required=False, action='store_true', help="Do no restore dump before this seed")
  seed_parser.add_argument('-size', required=True, type=int, help="Number of new requests to seed")
  seed_parser.add_argument('-clients', required=True, type=int, help="Number of clients to use for seeding")

  # workload application
  wkld_parser = subparsers.add_parser('wkld', help='Run workload generator')
  wkld_parser.add_argument('-c', '--clients', type=int, default=1, help="Number of clients to run on")
  wkld_parser.add_argument('-t', '--threads', type=int, default=1, help="Number of threads")
  wkld_parser.add_argument('-d', '--duration', type=int, default=1, help="Duration in seconds")
  wkld_parser.add_argument('-r', '--rate', type=int, default=999999, help="Work rate (throughput) in request per second")

  # ssh
  ssh_parser = subparsers.add_parser('ssh', help='SSH into machines')
  ssh_parser.add_argument('-hosts', required=True, nargs='+', help="Connect to nodes based on id")

  # parse args
  args = vars(main_parser.parse_args())
  command = args.pop('which')

  # parse deploy type
  args['deploy_type'] = None
  for dt in DEPLOY_TYPES:
    if args[dt]:
      args['deploy_type'] = dt
    del args[dt]

  # call parser method dynamically
  getattr(sys.modules[__name__], command)(args)