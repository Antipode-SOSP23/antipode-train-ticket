#!/usr/bin/env python3

from pathlib import Path
from pprint import pp
import os
import sys
import argparse
import time


#--------------
# GCP
#--------------
def _gcp_vm_create(zone, config):
  import googleapiclient.discovery
  import json

  compute = googleapiclient.discovery.build('compute', 'v1')
  try:
    compute.instances().insert(project=GCP_PROJECT_ID, zone=zone, body=config).execute()
    return _gcp_vm_get(zone=zone, name=config['name'])
  except googleapiclient.errors.HttpError as e:
    error_info = json.loads(e.args[1])['error']
    if error_info['code'] == 409 and error_info['errors'][0]['reason'] == 'alreadyExists':
      print(f"[WARN] Instance '{config['name']}' already exists")
      # get the existing instance details
      return _gcp_vm_get(zone=zone, name=config['name'])
    else:
      pp(error_info)
      exit(-1)

# Status available:
#   PROVISIONING, STAGING, RUNNING, STOPPING, SUSPENDING, SUSPENDED, REPAIRING, and TERMINATED
def _gcp_vm_wait_for_status(zone, name, status):
  import googleapiclient.discovery

  compute = googleapiclient.discovery.build('compute', 'v1')
  while True:
    r = _gcp_vm_get(zone=zone, name=name)
    if r['status'] == status.upper():
      return

def _gcp_vm_get(zone, name):
  import googleapiclient.discovery
  compute = googleapiclient.discovery.build('compute', 'v1')
  return compute.instances().get(project=GCP_PROJECT_ID, zone=zone, instance=name).execute()

def _gcp_vm_wait_for_ip(zone, name):
  while True:
    metadata = _gcp_vm_get(zone, name)
    try:
      network_interface = metadata['networkInterfaces'][0]
      return network_interface['accessConfigs'][0]['natIP'], network_interface['networkIP']
    except KeyError:
      time.sleep(1)
      pass


#--------------
# REST REQUESTS
#--------------
def create_contact(user_id, auth_token):
  from faker import Faker
  from faker.providers import ssn, phone_number
  import requests

  fake = Faker()

  url = f"{_service_public_endpoint('ts-contacts-service')}/api/v1/contactservice/contacts"
  headers = { 'Authorization': 'Bearer ' + auth_token }
  params = {
    "accountId": user_id,
    "name": fake.name(),
    "documentNumber": fake.ssn(),
    "documentType": 1, # ID Card
    "phoneNumber": fake.phone_number()
  }
  r = requests.post(url, headers=headers, json=params)
  return r.json()

def login_user(username, password):
  import requests

  # Login user
  url = f"{_service_public_endpoint('ts-auth-service')}/api/v1/users/login"
  params = {
    "username": "fdse_microservice",
    "password": "111111",
    "verificationCode": ""
  }
  r = requests.post(url, json=params)
  if r.status_code != 200:
    print("[ERROR] Failed to login user")
    exit(-1)

  out = r.json()
  return out['data']['userId'], out['data']['token']

def cancel_order(order_id, user_id, auth_token):
  import requests

  url = f"{_service_public_endpoint('ts-cancel-service')}/api/v1/cancelservice/cancel/{order_id}/{user_id}"
  headers = { 'Authorization': 'Bearer ' + auth_token }
  r = requests.get(url, headers=headers)
  if r.status_code != 200:
    print("[ERROR] Failed to cancel order")
    exit(-1)
  return r

def fetch_drawback(auth_token):
  import requests

  url = f"{_service_public_endpoint('ts-inside-payment-service')}/api/v1/inside_pay_service/inside_payment/money"
  headers = { 'Authorization': 'Bearer ' + auth_token }
  r = requests.get(url, headers=headers)
  if r.status_code != 200:
    print("[ERROR] Failed to fetch drawback")
    exit(-1)

  # if len([e for e in r.json()['data'] if e['orderId'] == order_id ]) == 0:
  #   print("INCONSISTENT")
  # else:
  #   print("CONSISTENT")

  return r


#--------------
# INFO
#--------------
def info(args):
  if args['links']:
    public_ip = _public_hostname()
    print(f"Homepage:\thttp://{public_ip}:8080\t\t\t(fdse_microservice:111111:1234)")
    print(f"Admin:\t\thttp://{public_ip}:8080/adminlogin.html\t(admin:222222)")
    print(f"Jaeger:\t\thttp://{public_ip}:16686")
    print(f"RabbitMQ:\thttp://{public_ip}:15672\t\t\t(guest:guest)")
    print(f"API Docs:")
    print(f"\tts-auth-service:\t\t{_service_public_endpoint('ts-auth-service')}/swagger-ui.html#!/user-controller/")
    print(f"\tts-contacts-service:\t\t{_service_public_endpoint('ts-contacts-service')}/swagger-ui.html#/contacts-controller")
    print(f"\tts-order-service:\t\t{_service_public_endpoint('ts-order-service')}/swagger-ui.html")
    print(f"\tts-cancel-service:\t\t{_service_public_endpoint('ts-cancel-service')}/swagger-ui.html#!/cancel-controller")
    print(f"\tts-inside-payment-service:\t{_service_public_endpoint('ts-inside-payment-service')}/swagger-ui.html#/inside-payment-controller")

  if args['ps']:
    from plumbum import local, FG
    from plumbum.cmd import docker_compose

    with local.cwd(APP_PATH):
      docker_compose['-f', DOCKER_COMPOSE_FILE, 'ps'] & FG

  if args['logs']:
    from plumbum import local, FG
    from plumbum.cmd import docker_compose

    with local.cwd(APP_PATH):
      docker_compose['-f', DOCKER_COMPOSE_FILE, 'logs', args['logs']] & FG


#--------------
# BUILD
#--------------
def build(args):
  getattr(sys.modules[__name__], f"build__{args.pop('deploy_type')}")(args)

def build__local(args):
  from plumbum import local, FG
  from plumbum.cmd import docker, docker_compose, find

  with local.cwd(APP_PATH):
    # remove target directories - maven was not cleaning
    find['.', '-type', 'd', '-name', 'target', '-exec', 'rm', '-rf', '{}', '+'] & FG
    # build from source
    # ref: https://github.com/FudanSELab/train-ticket/wiki/Installation-Guide
    mvn = docker['run', '--rm', '-ti',
      # keep cache between builds
      '-v', f"{os.environ['HOME']}/.m2:/root/.m2",
      # mount train ticket folder as working dir
      '-v', f"{APP_PATH}:/mnt/train-ticket",
      '-w', '/mnt/train-ticket',
      'maven:3-openjdk-8-slim',
      'mvn']
    mvn['clean', 'package', '-Dmaven.test.skip=true'] & FG
    time.sleep(1) # wait in between commands due to jar generation delay??
    docker_compose['build'] & FG

def build__gcp(args):
  # build local images first
  if not _is_inside_docker():
    build__local(args)

  # then force docker
  _force_gcp_docker()
  import googleapiclient.discovery
  from plumbum import local, FG
  from plumbum.cmd import gcloud, docker
  from dotenv import dotenv_values
  import json

  # tag images built localy with GCP tag
  with local.cwd(APP_PATH):
    config = dotenv_values(".env")
    built_images = { s.split('|')[0] : s.split('|')[1] for s in docker['images', f"{config['NAMESPACE']}/*", '--format', "{{.ID}}|{{.Repository}}"]().split('\n') if s}
    for image_id, image_name in built_images.items():
      base_image_name = image_name.split('/')[-1]
      gcp_image_name = f"{GCP_DOCKER_IMAGE_NAMESPACE}/{base_image_name}:{GCP_DOCKER_IMAGE_TAG}"
      docker['tag', image_id, gcp_image_name] & FG
      docker['push', gcp_image_name] & FG
      docker['rmi', gcp_image_name] & FG

  # Create base VM
  compute = googleapiclient.discovery.build('compute', 'v1')
  image_response = compute.images().getFromFamily(project='debian-cloud', family='debian-10').execute()
  source_disk_image = image_response['selfLink']
  name = 'antipode-dev-tt'
  zone = 'us-east1-b'
  machine_type = 'f1-micro'
  config = {
    'name': name,
    'machineType': f"zones/{zone}/machineTypes/{machine_type}",
    'disks': [
      {
        'boot': True,
        'autoDelete': True,
        'initializeParams': {
          'sourceImage': source_disk_image,
        }
      }
    ],
    'hostname': 'antipode-dev.tt',
    # Specify a network interface with NAT to access the public internet.
    'networkInterfaces': [
      {
        'network': 'global/networks/default',
        'accessConfigs': [
          {'type': 'ONE_TO_ONE_NAT', 'name': 'External NAT'}
        ]
      }
    ],
    # tags for firewall rules
    "tags": {
      "items": [],
    },
  }
  r = _gcp_vm_create(zone, config)
  # start the vm if not running
  compute.instances().start(project=GCP_PROJECT_ID, zone=zone, instance=r['id']).execute()
  _gcp_vm_wait_for_status(zone, r['id'], 'RUNNING')
  time.sleep(10) # extra time to avoid ssh errors

  # In order to get the .ssh folder
  # 1) Start a local GCP container
  # 2) Go the the GCP console, and in the SSH context menu, copy the gcloud command, it will look something like:
  #    $ gcloud beta compute ssh --zone "europe-west3-c" "antipode-dev-tt" --project "pluribus"
  # 3) Paste the above command in the container and let it run
  # 4) Copy the files from the container to outside
  #       docker cp <CONTAINER ID>:/root/.ssh/google_compute_engine .
  #       docker cp <CONTAINER ID>:/root/.ssh/google_compute_engine.pub .

  # copy and execute base_vm script
  gcloud['compute', 'scp', 'gcp/base_vm.sh', 'gcp/pluribus.json', f"{GCP_DEFAULT_SSH_USER}@{name}:/tmp/"] & FG
  gcloud['compute', 'ssh', f"{GCP_DEFAULT_SSH_USER}@{name}", '--command', 'bash /tmp/base_vm.sh'] & FG

  # stop machine
  compute.instances().stop(project=GCP_PROJECT_ID, zone=zone, instance=r['id']).execute()
  _gcp_vm_wait_for_status(zone, r['id'], 'TERMINATED')

  # built image based on the image of this machine
  image_config = {
    "kind": "compute#image",
    "name": GCP_MACHINE_IMAGE_NAME,
    "sourceDisk": "projects/pluribus/zones/us-east1-b/disks/antipode-dev-tt",
    "storageLocations": [
      "us"
    ]
  }

  # check if there is already an image - delete if necessary
  try:
    compute.images().get(project=GCP_PROJECT_ID, image=image_config['name']).execute()
    # if no exception delete the image
    print("[INFO] Image already exists - deleting!")
    compute.images().delete(project=GCP_PROJECT_ID, image=image_config['name']).execute()
    # wait for image to be deleted
    while True:
      # 404 exceptions will result into going to except and pass - means we deleted the image sucessfully
      compute.images().get(project=GCP_PROJECT_ID, image=image_config['name']).execute()
  except googleapiclient.errors.HttpError as e:
    error_info = json.loads(e.args[1])['error']
    if error_info['code'] == 404:
      pass
    else:
      raise(e)

  # create the new image
  compute.images().insert(project=GCP_PROJECT_ID, body=image_config).execute()
  # wait for image to be ready
  while True:
    if compute.images().get(project=GCP_PROJECT_ID, image=image_config['name']).execute()['status'] == 'READY':
      break

  # find all used ports
  firewall_rules = compute.firewalls().list(project=GCP_PROJECT_ID, filter=None).execute()['items']
  for frule in firewall_rules:
    if frule['name'] == 'portainer':
      print("[INFO] Firewall rule for portainer found")
    elif frule['name'] == 'swarm':
      print("[INFO] Firewall rule for swarm found")
    elif frule['name'] == 'nodes':
      print("[INFO] Firewall rule for nodes found")
      tcp_rule = [ e for e in frule['allowed'] if e['IPProtocol'] == 'tcp' ][0]
      ports = [ str(e) for e in sorted(set([ int(r) for r in tcp_rule['ports'] ])) ]
      # check ports
      docker_compose_ports = [ e.split(':')[0] for e in _flat_list([ sinfo['ports'] for sname, sinfo in _load_yaml(APP_PATH / 'docker-compose.yml')['services'].items() if 'ports' in sinfo]) ]
      if (set(docker_compose_ports) - set(ports)):
        print("[ERROR] Missing ports on firewall rule!")
        exit(-1)

#--------------
# DEPLOY
#--------------
def deploy(args):
  getattr(sys.modules[__name__], f"deploy__{args.pop('deploy_type')}")(args)

def deploy__local(args):
  return

def deploy__gcp(args):
  _force_gcp_docker()
  from plumbum import local, FG
  from plumbum.cmd import ansible_playbook
  from datetime import datetime
  from jinja2 import Environment
  import textwrap

  _put_last('deploy_config', args['config'])
  # TODO: Load tag from .last.file when doing run or wkld gcp
  tag = f"{datetime.now().strftime('%Y%m%d%H%M%S')}"
  _put_last('deploy_tag', tag)

  # create deploy folder
  deploy_path = ROOT_PATH / 'deploy' / tag
  os.makedirs(deploy_path, exist_ok=True)

  # load the config and the local compose
  config = _load_yaml(args['config'])
  compose = _load_yaml(DOCKER_COMPOSE_FILE)

  # Set external network driver
  compose['networks'][DOCKER_COMPOSE_NETWORK]['external'] = { 'name': DOCKER_COMPOSE_NETWORK }

  # replace swarm info onto existing docker-compose
  for service, node_id in config['services'].items():
    # replace the deploy constraints
    sinfo = compose['services'][service]
    if 'deploy' not in sinfo: sinfo['deploy'] = {}
    if 'placement' not in sinfo['deploy']: sinfo['deploy']['placement'] = {}
    if 'constraints' not in sinfo['deploy']['placement']: sinfo['deploy']['placement']['constraints'] = []
    # get the index id of constraint of the node hostname
    deploy_constraints = sinfo['deploy']['placement']['constraints']
    hostname_index = _list_index_containing_substring(deploy_constraints, 'node.hostname')
    # replace hostname constraint
    deploy_constraints.insert(hostname_index, f"node.hostname == {node_id}")

  for _, service_info in compose['services'].items():
    if 'image' in service_info:
      if '${NAMESPACE}' in service_info['image']:
        service_info['image'] = service_info['image'].replace('${NAMESPACE}', GCP_DOCKER_IMAGE_NAMESPACE).replace('${TAG}', GCP_DOCKER_IMAGE_TAG)

  # now write the new compose into a new file
  _dump_yaml(deploy_path / 'docker-compose.yml', compose)
  print(f"[SAVED] '{deploy_path / 'docker-compose.yml'}'")

  # Keep track of all created vms
  vms = { 'swarm_manager': [], 'cluster': [] }

  # manager
  manager_config = {
    'name': config['manager']['name'],
    'machineType': f"zones/{config['manager']['zone']}/machineTypes/{config['manager']['machine_type']}",
    'disks': [
      {
        'boot': True,
        'autoDelete': True,
        'initializeParams': {
          'sourceImage': GCP_MACHINE_IMAGE_LINK,
        }
      }
    ],
    'hostname': config['manager']['hostname'],
    # Specify a network interface with NAT to access the public internet.
    'networkInterfaces': [
      {
        'network': 'global/networks/default',
        'accessConfigs': [
          {'type': 'ONE_TO_ONE_NAT', 'name': 'External NAT'}
        ]
      }
    ],
    # tags for firewall rules
    "tags": {
      "items": ['swarm', 'portainer'],
    },
  }
  vms['swarm_manager'].append(_gcp_vm_create(config['manager']['zone'], manager_config))

  # app nodes
  for node_name, node_info in config['nodes'].items():
    node_config = {
      'name': node_name,
      'machineType': f"zones/{node_info['zone']}/machineTypes/{node_info['machine_type']}",
      'disks': [
        {
          'boot': True,
          'autoDelete': True,
          'initializeParams': {
            'sourceImage': GCP_MACHINE_IMAGE_LINK,
          }
        }
      ],
      'hostname': node_info['hostname'],
      # Specify a network interface with NAT to access the public internet.
      'networkInterfaces': [
        {
          'network': 'global/networks/default',
          'accessConfigs': [
            {'type': 'ONE_TO_ONE_NAT', 'name': 'External NAT'}
          ]
        }
      ],
      # tags for firewall rules
      "tags": {
        "items": ['swarm', 'nodes'],
      },
    }
    vms['cluster'].append(_gcp_vm_create(node_info['zone'], node_config))

  # Build inventory for ansible playbooks
  inventory = {}
  for group_name, group_nodes in vms.items():
    for v in group_nodes:
      external_ip, internal_ip = _gcp_vm_wait_for_ip(node_info['zone'], v['name'])

      if group_name not in inventory:
        inventory[group_name] = []
      inventory[group_name].append({
        'name': v['name'],
        'hostname': v['hostname'],
        'zone': v['zone'].split('/')[-1],
        'external_ip': external_ip,
        'internal_ip': internal_ip,
      })

  # now build the inventory
  # if you want to get a new google_compute_engine key:
  #   1) go to https://console.cloud.google.com/compute/instances
  #   2) choose one instance, click on SSH triangle for more options
  #   3) 'View gcloud command'
  #   4) Run that command and then copy the generated key
  #
  template = """
    [swarm_manager]
    {{ swarm_manager['hostname'] }} ansible_host={{ swarm_manager['external_ip'] }} gcp_zone={{ swarm_manager['zone'] }} gcp_name={{ swarm_manager['name'] }} gcp_host={{ swarm_manager['internal_ip'] }} ansible_user=root ansible_ssh_private_key_file={{ private_key_path }}


    [cluster]
    {% for node in deploy_nodes %}{{ node['hostname'] }} ansible_host={{ node['external_ip'] }} gcp_zone={{ node['zone'] }} gcp_name={{ node['name'] }} gcp_host={{ node['internal_ip'] }} ansible_user=root ansible_ssh_private_key_file={{ private_key_path }}{% endfor %}
  """
  inventory = Environment().from_string(template).render({
    'swarm_manager': inventory['swarm_manager'][0], # only one swarm manager
    'deploy_nodes': inventory['cluster'],
    'private_key_path': str(ROOT_PATH / 'gcp' / '.ssh' / 'google_compute_engine'),
  })
  inventory_filepath = deploy_path / 'inventory.cfg'
  with open(inventory_filepath, 'w') as f:
    # remove empty lines and dedent for easier read
    f.write(textwrap.dedent(inventory))
  print(f"[SAVED] '{inventory_filepath}'")

  # sleep to give extra time for nodes to be ready to accept requests
  time.sleep(30)

  # run playbooks
  with local.cwd(ROOT_PATH / 'gcp'):
    if args['prometheus']:
      ansible_playbook['deploy-prometheus.yml',
        '-i', inventory_filepath
      ] & FG


    # deploy swarm
    ansible_playbook['deploy-swarm.yml',
      '-i', inventory_filepath,
      '-e', f"swarm_overlay_network={DOCKER_COMPOSE_NETWORK}",
    ] & FG

  exit()

#--------------
# RUN
#--------------
def run(args):
  from plumbum import local, FG
  from plumbum.cmd import docker_compose

  with local.cwd(APP_PATH):
    with local.env(COMPOSE_HTTP_TIMEOUT=180):
      docker_compose['-f', DOCKER_COMPOSE_FILE, 'up'] & FG


#--------------
# CLEAN
#--------------
def clean(args):
  from plumbum import local, FG
  from plumbum.cmd import docker, docker_compose


  with local.cwd(APP_PATH):
    docker_compose['-f', DOCKER_COMPOSE_FILE, 'down'] & FG
    # cleaning system because train consumes a lot of memory/disk
    docker['system', 'prune', '--volumes', '-f'] & FG


#--------------
# WORKLOAD
#--------------
def wkld(args):
  from datetime import datetime
  from jinja2 import Environment
  import textwrap
  from plumbum import FG
  from plumbum import local
  from plumbum.cmd import docker_compose
  import shutil

  # tag for this deployment
  tag = f"{datetime.now().strftime('%Y%m%d%H%M%S')}"

  # create deploy folder
  deploy_path = ROOT_PATH / 'deploy' / tag
  os.makedirs(deploy_path, exist_ok=True)

  # copy locust files to deploy folder
  shutil.copy(ROOT_PATH / 'locust' / f"locustfile.py", deploy_path)
  shutil.copy(ROOT_PATH / 'locust' / f"Dockerfile", deploy_path)

  # dynamically build docker-compose for clients
  public_ip = _public_hostname()
  template = """
    version: '3'
    services:
      master:
        build: .
        working_dir: /mnt/locust
        command: -f locustfile.py --master -H http://master:8089 --expect-workers {{ num_workers }} --headless --reset-stats --only-summary --logfile {{ report_prefix }}.err --html {{ report_prefix }}.html --csv {{ report_prefix }} --run-time {{ run_time }} -u {{ users }}
        environment:
          AUTH_SERVICE_URL: {{ auth_service_url }}
          ORDER_SERVICE_URL: {{ order_service_url }}
          CANCEL_SERVICE_URL: {{ cancel_service_url }}
          INSIDE_PAYMENT_SERVICE_URL: {{ inside_payment_service_url }}
          USERNAME: {{ username }}
          PASSWORD: {{ password }}
          WORKER_ID: MASTER
          SEED_SIZE: {{ seed_size }}
        ports:
          - "8089:8089"
        volumes:
          - ./:/mnt/locust

      {% for worker_id in range(num_workers) %}
      worker_{{ worker_id }}:
        build: .
        working_dir: /mnt/locust
        command: -f locustfile.py --worker --master-host master --headless --reset-stats
        environment:
          AUTH_SERVICE_URL: {{ auth_service_url }}
          ORDER_SERVICE_URL: {{ order_service_url }}
          CANCEL_SERVICE_URL: {{ cancel_service_url }}
          INSIDE_PAYMENT_SERVICE_URL: {{ inside_payment_service_url }}
          USERNAME: {{ username }}
          PASSWORD: {{ password }}
          WORKER_ID: {{ worker_id }}
          SEED_SIZE: {{ seed_size }}
        volumes:
          - ./:/mnt/locust
      {% endfor %}
  """
  inventory = Environment().from_string(template).render({
    'auth_service_url': _service_public_endpoint('ts-auth-service'),
    'order_service_url': _service_public_endpoint('ts-order-service'),
    'cancel_service_url': _service_public_endpoint('ts-cancel-service'),
    'inside_payment_service_url': _service_public_endpoint('ts-inside-payment-service'),
    'username': 'fdse_microservice',
    'password': '111111',
    'num_workers': args['clients'],
    'users': args['threads'],
    'run_time': f"{args['duration']}s",
    'report_prefix': tag,
    'seed_size': SEED_TO_DURATION_FACTOR * args['duration'],
  })
  docker_compose_filepath = deploy_path / 'docker-compose.yml'
  with open(docker_compose_filepath, 'w') as f:
    # remove empty lines and dedent for easier read
    f.write(textwrap.dedent(inventory))
  print(f"[SAVED] '{docker_compose_filepath}'")

  # run locust and remove containers
  with local.cwd(deploy_path):
    docker_compose['up'] & FG
    # save log from master in
    (docker_compose['logs', '--no-color', '--no-log-prefix', 'master'] > f"master.out" )& FG
    # save log from workers
    for worker_id in range(args['clients']):
      (docker_compose['logs', '--no-color', '--no-log-prefix', f"worker_{worker_id}"] > f"worker_{worker_id}.out" )& FG
    # remove and clean containers
    docker_compose['down', '--rmi', 'local', '-v'] & FG

  # create gather folder
  gather_path = ROOT_PATH / 'gather' / tag
  os.makedirs(gather_path, exist_ok=True)
  # now move the gather files to the gather directory
  shutil.move(deploy_path / f"{tag}.err", gather_path / f"master_log.err")
  shutil.move(deploy_path / f"master.out", gather_path / f"master.out")
  for worker_id in range(args['clients']):
    shutil.move(deploy_path / f"worker_{worker_id}.out", gather_path / f"worker_{worker_id}.out")
  shutil.move(deploy_path / f"{tag}.html", gather_path / f"report.html")
  shutil.move(deploy_path / f"{tag}_exceptions.csv", gather_path / f"exceptions.csv")
  shutil.move(deploy_path / f"{tag}_failures.csv", gather_path / f"failures.csv")
  shutil.move(deploy_path / f"{tag}_stats_history.csv", gather_path / f"stats_history.csv")
  shutil.move(deploy_path / f"{tag}_stats.csv", gather_path / f"stats.csv")


#--------------
# HELPERS
#--------------
def _services():
  import yaml

  with open(DOCKER_COMPOSE_FILE) as file:
    return list(yaml.load(file, Loader=yaml.FullLoader)['services'].keys())

def _services_to_port():
  import yaml

  with open(DOCKER_COMPOSE_FILE) as file:
    return { name:int(d['ports'][0].split(':')[0]) for name,d in yaml.load(file, Loader=yaml.FullLoader)['services'].items() if 'ports' in d }

def _load_yaml(path):
  import yaml
  with open(path, 'r') as f:
    return yaml.safe_load(f) or {}

def _dump_yaml(path, d):
  import yaml
  with open(path, 'w+') as f:
    yaml.safe_dump(d, f, default_flow_style=False)

def _put_last(k,v):
  import yaml
  doc = {}
  # if file exists parse yaml otherwise create empty dict to write to
  if Path(LAST_INFO_FILE).exists():
    doc = _load_yaml(LAST_INFO_FILE)
  # write new value and save to file
  doc[k] = v
  _dump_yaml(LAST_INFO_FILE, doc)

def _get_last(k):
  import yaml
  doc = _load_yaml(LAST_INFO_FILE)
  return doc.get(k)

def _split_list(a, n):
    k, m = divmod(len(a), n)
    return list(a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))

def _flat_list(l):
  return [item for sublist in l for item in sublist]

def _list_index_containing_substring(the_list, substring):
  for i, s in enumerate(the_list):
    if substring in s:
      return i
  return 0

def _public_hostname():
  from plumbum.cmd import hostname
  return hostname['-I']().split()[1]

def _service_public_endpoint(name):
  return f"http://{_public_hostname()}:{SERVICES_TO_PORT[name]}"

def _is_inside_docker():
  return os.path.isfile('/.dockerenv')

def _force_gcp_docker():
  if _is_inside_docker():
    return

  from plumbum import local, FG
  from plumbum.cmd import docker
  import subprocess

  # if image is not built, we do it
  if docker['images', GCP_DOCKER_IMAGE_NAME, '--format', '"{{.ID}}"']().strip() == '':
    with local.cwd(ROOT_PATH / 'gcp'):
      docker['build', '-t', GCP_DOCKER_IMAGE_NAME, '.'] & FG

  args = list()
  args.extend(['docker', 'run', '--rm', '-it',
    # run docker from host inside the container
    '-v', '/var/run/docker.sock:/var/run/docker.sock',
    '-v', '/usr/bin/docker:/usr/bin/docker',
    '-v', '/usr/bin/docker-compose:/usr/bin/docker-compose',
    # volumes
    '-v', f"{ROOT_PATH}:/mnt/code",
    '-v', f"{ROOT_PATH / 'gcp' / '.ssh'}:/root/.ssh",
    '-w', '/mnt/code',
    #
    GCP_DOCKER_IMAGE_NAME
  ])
  args = args + sys.argv
  # print(' '.join(args))
  subprocess.call(args)
  exit()


#--------------
# CONSTANTS
#--------------
SCRIPT_NAME = __file__.rsplit("/", 1)[1].split('.')[0]
ROOT_PATH = Path(os.path.abspath(os.path.dirname(sys.argv[0])))
APP_PATH = ROOT_PATH / 'train-ticket'
DOCKER_COMPOSE_FILE = APP_PATH / 'docker-compose.yml'
DOCKER_COMPOSE_NETWORK = 'my-network'
SERVICES = _services()
SERVICES_TO_PORT = _services_to_port()
LAST_INFO_FILE = ROOT_PATH / '.last.yml'
SEED_TO_DURATION_FACTOR = 50 # in 1 second it does less than this
DEPLOY_TYPES = [ 'local', 'gcp' ]
GCP_PROJECT_ID = 'pluribus'
GCP_DOCKER_IMAGE_NAME = 'gcp-manager:antipode'
GCP_DOCKER_IMAGE_NAMESPACE = f"gcr.io/{GCP_PROJECT_ID}"
GCP_DOCKER_IMAGE_TAG = "antipode"
GCP_DEFAULT_SSH_USER = 'jfloff'
GCP_MACHINE_IMAGE_NAME = 'antipode-tt'
GCP_MACHINE_IMAGE_LINK = f"https://www.googleapis.com/compute/v1/projects/{GCP_PROJECT_ID}/global/images/{GCP_MACHINE_IMAGE_NAME}"


#--------------
# CLI
#--------------
if __name__ == '__main__':
  # parse arguments
  main_parser = argparse.ArgumentParser()

  # deploy flag
  deploy_type_group = main_parser.add_mutually_exclusive_group(required=True)
  for dt in DEPLOY_TYPES:
    deploy_type_group.add_argument(f"--{dt}", action='store_true')

  # different commands
  subparsers = main_parser.add_subparsers(help='commands', dest='which')

  # build application
  build_parser = subparsers.add_parser('build', help='Build application from source')

  # deploy application
  deploy_parser = subparsers.add_parser('deploy', help='Deploy application')
  deploy_parser.add_argument('-config', help="Deploy configuration")
  deploy_parser.add_argument('-prometheus', action='store_true', help="Deploy configuration")

  # info application
  info_parser = subparsers.add_parser('info', help='Application info')
  info_parser.add_argument('-links', action='store_true', help="Application links")
  info_parser.add_argument('-ps', action='store_true', help="Application processes")
  info_parser.add_argument('-logs', choices=SERVICES, help="Application processes")

  # run application
  run_parser = subparsers.add_parser('run', help='Run application')

  # clean application
  clean_parser = subparsers.add_parser('clean', help='Clean application')

  # workload application
  seed_parser = subparsers.add_parser('seed', help='Seed orders')
  seed_parser.add_argument('-no', '--num-orders', type=int, default=1, help="Number or orders to spawn")

  # workload application
  wkld_parser = subparsers.add_parser('wkld', help='Run workload generator')
  wkld_parser.add_argument('-c', '--clients', type=int, default=1, help="Number of clients to run on")
  wkld_parser.add_argument('-t', '--threads', type=int, default=1, help="Number of threads")
  wkld_parser.add_argument('-d', '--duration', type=int, default='1', help="Duration in seconds")
  # wkld_parser.add_argument('-r', '--rate', type=int, default=1, help="Work rate (throughput) in request per second")

  # parse args
  args = vars(main_parser.parse_args())
  command = args.pop('which')

  # parse deploy type
  args['deploy_type'] = None
  for dt in DEPLOY_TYPES:
    if args[dt]:
      args['deploy_type'] = dt
    del args[dt]

  # call parser method dynamically
  getattr(sys.modules[__name__], command)(args)